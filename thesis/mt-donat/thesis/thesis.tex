%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%
%           P R E A M B L E             %
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%


% Package configuration
%\PassOptionsToPackage{biblatex}{style=ieee, backend=biber}
%\PassOptionsToPackage{glossaries}{}


% Document Class ===============================================================
\def\PathToTumTemplate{.}
\PassOptionsToClass{ebook}{tum_thesis}
\documentclass[%
    %ebook   % create digital ebook
    ]{\PathToTumTemplate/thesis/tum_thesis}



% thesis settings
% -------------------------------------------------------------------
\RequirePackage[english]{babel}   % or 'ngerman', delete .aux after changeing




% thesis information
% -------------------------------------------------------------------
\title{Implementation and Evaluation of a Reputation Framework to Increase Reputation Accuracy}                          % defines \thetitle
\author{Márton Donát Nagy}                         % defines \theauthor
\myemail{mdonat.nagy@tum.de}                 % defines \theemail
\authorsaddress{Schwere-Reiter-Str. 35./420 \\ 80797 Munich}
\studentnumber{03699798}


\doctype{Master's Thesis}
\professor{Prof. Dr. phil. nat. Sebastian Steinhorst}
\advisor{Emanuel Regnath}
\coadvisor{Corinna Coadvisor}
\submitdate{\today}



% chair information
% -------------------------------------------------------------------
\RequirePackage{\PathToTumTemplate/tum/ei/esi/esi}  % your chair
\RequirePackage{\PathToTumTemplate/tum/tum_header} 
\logoheader{\vspace*{-3cm}\printTumHeader}


% listing language
\lstset{ language=ada }

% other packages
% -------------------------------------------------------------------
\RequirePackage[automake=true, toc, acronym]{glossaries}
\setacronymstyle{long-short}
%\makenoidxglossaries
\makeglossaries
\RequirePackage{listings}
% \begin{lstlisting} 
% 	code here...
% \end{lstlisting}
\RequirePackage{graphicx}
% \includegraphics[width=\columnwidth]{img/uml_spec}
\RequirePackage{siunitx}
% \SI{3.14}{\meter\per\second}
\RequirePackage{microtype}          % improved kerning, layout
\RequirePackage{hyperref}           % clickable references
\RequirePackage{booktabs}           % professional tables
\RequirePackage{multirow}
\RequirePackage{caption}
\RequirePackage{amsmath}

% use \emph{} for special terms, use \texttt{} for general code variables, and \lstinline{} for code.
% use \textbf{} for heading cells in tables
% use \mathrm{} in math environment for things that are not variables, like function names.
% use \SI{3.14}{\meter\per\second} for physical quantities (requires \RequirePackage{siunitx})
% use \gls{RAM} for acronyms. This feels annoying but will take care of everything and allows you to copy paste text without worrying.


% abbreviations / acronyms
% -------------------------------------------------------------------
\newacronym{ctai}{CTAI}{Claim Truth Assessment Inaccuracy}
%\newacronym[shortplural={BLKs},longplural={Belastungskollektive}]{BLK}{BLK}{Belastungskollektiv}
\newacronym{trsys}{TR System}{trust and reputation system}
\newacronym{rs}{RS}{reputation system}
\newacronym{ts}{TS}{Trust System}
\newacronym{p2p}{p2p}{peer-to-peer}
\newacronym{eta}{ETA}{estimated time of arrival}
\newacronym{iot}{IoT}{internet of things}
\newacronym{cps}{CPS}{cyber-physical system}
\newacronym[shortplural={MAS}]{mas}{MAS}{multi-agent system}

% glossary entries
% -------------------------------------------------------------------
\newglossaryentry{scenario}
{
	name={scenario},
	description={Description of a complete simulation environment for Pyrepsys. Specifies the reputation scheme, improvement methods, agents, possible review values etc. See Section~\ref{sec:scenario} for details}
}
\newglossaryentry{claim}
{
	name={claim},
	description={The representation of a content provided by an agent. See sec.~\ref{sec:approach_claims}}
}
\newglossaryentry{review}
{
	name={review},
	description={Represents a quality valuation of a content (claim). Also called regular review. See sec.~\ref{sec:approach_reviews}}
}
\newglossaryentry{author_review}
{
	name={author review},
	description={A claimer's quality valuation of its own content. See sec.~\ref{sec:approach_author_review}}
}
\newglossaryentry{gr_truth}
{
	name={ground truth},
	description={The real and hidden objective quality of a content. Unknown to agents. See sec.~\ref{sec:approach_gr_truth}}
}
\newglossaryentry{score}
{
	name={score},
	description={Refers to a reputation, regular rating or author rating's value}
}
\newglossaryentry{claim_probability}
{
	name={claiming probability},
	description={A percent likelihood determining whether an agent initiates a claiming process if given an opportunity. Specified in scenarios for each agent}
}
\newglossaryentry{rate_probability}
{
	name={rating probability},
	description={A percent likelihood determining whether an agent leaves a review on a claim if given an opportunity. Specified in scenarios for each agent}
}
\newglossaryentry{main_rng_chain}
{
	name={main random chain},
	description={Top-level sequence of random numbers. See sec.~\ref{sec:impl_rng}}
}
\newglossaryentry{branchoff_rng_chain}
{
	name={branch-off random chain},
	description={A secondary sequence of random numbers that is thrown away after it is used. Seeded with a random value from the main chain. See sec.~\ref{sec:impl_rng}}
}
\newglossaryentry{measured_claim_score}
{
	name={measured claim quality},
	description={An agent's approximation of a claim's ground truth. See sec.~\ref{sec:approach_measure_claim}}
}
\newglossaryentry{claim_range}
{
	name={claim range},
	description={Claim author reviews, reviews, ground truths, measured qualities and reputations fall within this range}
}
\newglossaryentry{distort_strategy}
{
	name={distortion strategy},
	description={Claimers calculation method of author review values. See sec.~\ref{sec:distort_strategies}}
}
\newglossaryentry{rate_strategy}
{
	name={rating strategy},
	description={Agents calculation method of review values. See sec.~\ref{sec:approach_rating_strategies}}
}
\newglossaryentry{distorted_claim_quality}
{
	name={distorted claim quality},
	description={The value produced by the distort strategy from the measured claim quality}
}
\newglossaryentry{rating_span}
{
	name={rating~span},
	description={The size of the rating range, calculated as the difference of the maximum and minimum review values}
}
\newglossaryentry{i_representation}
{
	name={internal representation},
	description={Refers to storing values in the [0,1] range by Pyrepsys. Meant for internal use in data structures. See \gls{ae_representation}, sec.~\ref{sec:i_ae_reprepentation}}
}
\newglossaryentry{ae_representation}
{
	name={agent-exposed representation},
	description={Refers to storing values in the configured rating range [minimum~rating,~maximum~rating] in Pyrepsys. This is what agents and users see. See \gls{i_representation}, sec.~\ref{sec:i_ae_reprepentation}}
}
\newglossaryentry{artifacts_dir}
{
	name={artifacts directory},
	description={Directory in which the logfile and simulation results from metrics are saved. Located in the \texttt{simulation\_artifacts} directory. Named after the date and time Pyrepsys was invoked.}
}


% glossary and acro usage
% -------------------------------------------------------------------
% \gls{maths} \glspl{formula} Gls Glspl to capitalize
% \acrlong{gcd}, \acrshort{gcd} \acrfull{lcm} \~pl()


\newtheorem{definition}{Definition}[chapter]



% document begin (output starts here)
% -------------------------------------------------------------------
\begin{document}
\frontmatter

% titlepage
\maketitle

% Colophon
\newcommand{\thecolophon}{  
    \begin{colophon}
        \vspace*{1cm}     
        \begin{minipage}{0.5\textwidth}\begin{flushleft}
        This thesis was typeset using the XeTeX{} 
        typesetting system developed by Jonathan Kew. 
        \end{flushleft}
        \end{minipage}
    \end{colophon}
}
%\thecolophon   % uncomment to print colophon (optional)
%\cleardoublepage


% declaration
\begin{authordecl}
    \noindent I, \theauthor, declare that this thesis titled
    ``\thetitle'' and the work presented in it are my own unaided
    work, and that I have acknowledged all direct or indirect sources as
    references.

    This thesis was not previously presented to another examination board
    and has not been published.

    \vspace{2em}

    \noindent Signed:\\\vspace{1em}
    \noindent\rule[0.5em]{25em}{0.5pt} % This prints a line for the signature
     
    \noindent Date:\\\vspace{1em}
    \noindent\rule[0.5em]{25em}{0.5pt} % This prints a line to write the date
    \rmfamily
\end{authordecl}
\cleardoublepage


% abstract
%Abstract: Short summary of the context, the problem, your approach and its experimental results (numbers!).
\begin{abstract}
	The advent of technologically scaled transactional spaces and the explosion of user numbers in them raises the problem of who participants should trust.
	Along human oriented systems like online marketplaces, algorithmic environments are becoming more and more prevalent, where participating agents are autonomous machines.
	Such domains are the Internet of Things and autonomous driving.
	
	Trust and reputation research aims to provide systems to solve the trust problem.
	The research field is divided as countless approaches have been proposed in various domains.
	Many approaches repeat certain patterns to increase reputation accuracy and make the system more resistant against malicious attacks.
	Our goal is to collect and evaluate the usefulness of these building blocks.
	
	We propose a generic model for reputation systems, and using it we implement a Python reputation evaluation framework called Pyrepsys.
	To our knowledge, Pyrepsys is the only evaluation framework to support modular reputation improvement methods.

	We collected promising improvement approaches from literature.
	From these, we evaluated the use of aging and weights.
	Experiments showed that aging makes less sense when agent behavior never changes.
	For weights to work, the reputation calculation needs to give reputation to pure rater agents as well.
	
	Finally, we identified further ways to improve the Pyrepsys system.
\end{abstract}



% table of contents
\setcounter{tocdepth}{1}
\tableofcontents



\cleardoublepage
\mainmatter


% CHAPTER ######################################################################
\chapter{Introduction}
% ##############################################################################
%Introduction: Draw a picture of the context which your work is embedded.
%    Motivation: Describe the ultimate goal, the glory future that we want to reach.
%    Problem Statement: Why is there currently a gap between us and our goal? Why is SotA not sufficient? What is the problem?
%    Contributions: What will you do / have you done to narrow this gap? List 3-5 contributions as bullet points.
 
% what is the problem
% what can u do w rep systems
% goal is to improve accuracy bc u also have biased rated etc

\lettrine{T}{rust} and reputation are an inherent part of our biology and social behavior, helping us navigate human-scaled communities.
With the advent of technologically scaled transactional spaces, the problem of who to trust quickly becomes an issue, as in an online marketplace for example.
Similar problems also arise in networks partly or fully used by machines, such as in decentralized Internet of Things or autonomous driving.

Cryptographic methods can guarantee the identity of agents and the integrity of their communication and actions.
They cannot, however, tell if what agents say is true or reliable, or whether they act in good faith or not.
Trust and reputation systems aim to fulfill this purpose.



% --------------------------------------------------------------
\section{Problem Statement}\label{sec:probstat}
% --------------------------------------------------------------
Countless reputation algorithms and reputation system evaluation frameworks are published in the literature.
The problem someone setting up a reputation system faces is the overabundance of methods, and the lack of domain-independent comparison.

Existing reputation systems already contain building blocks that can be reused in a different system to improve its reputation estimate's accuracy.
We would like to have a commonly available collection of such improvement techniques, along with their potency of improving reputation methods.
We also see the need for a generic evaluation framework that can be used to compare improvement techniques and select which methods make sense to use in various applications.
Currently no such system exists.



% --------------------------------------------------------------
\section{Scenario}
% --------------------------------------------------------------

We aim to stay general in terms of domains and environments.
There is however a prioritization of algorithmic environments like Internet of Things, more specifically intersection management in autonomous driving.
While it serves as a recurring example throughout this thesis, the models and simulations are not at all limited to this specific use case.
Methods and experiments are intended to be general enough that they apply for \gls{iot}, \gls{cps}, or any other application as well.

The autonomous driving example assumes a hypothetical algorithm governing intersections, regulating the order vehicles pass crossways.
Vehicles are assumed to provide an \gls{eta} to the intersection, based on which the passing order is determined.
Vehicle agents may be tempted to report a shorter \gls{eta} to gain an earlier right of way priority.
The use case for a reputation system is to keep these attempts in check by giving each vehicle agent a reputation.




% --------------------------------------------------------------
\section{Task Definition}
% --------------------------------------------------------------

We will

\begin{itemize}
    \item take a survey of reputation methods
    \item identify independent patterns in literature which can be used as improvement techniques
    \item take a survey of reputation evaluation frameworks
    \item see if any published reputation evaluation framework can be used or adapted to compare improvements
    \item or if not, design and implement a new evaluation system
\end{itemize}



% --------------------------------------------------------------
\section{Structure of This Document}
% --------------------------------------------------------------

First, Chapter~\ref{chap:related_work} covers the basics of trust and reputation systems, and explores existing reputation improvement techniques and published evaluation frameworks.
Chapter~\ref{chap:approach} describes the approach used to model and compare reputation improvements.
Then, Chapter~\ref{chap:implementation} details the implementation of the introduced model.
Results of simulations made with the implemented simulator are shown and discussed in Chapter~\ref{chap:evaluation}.
Finally, Chapter~\ref{chap:conclusion} concludes this thesis.



%% --------------------------------------------------------------
%\section{Terminology}
%% --------------------------------------------------------------

%describe user, peer, agent, principal, node etc namings
%in research used according to domain of the paper

%network

%underlying value, trueness, ground truth, 

%reputation score, estimate, global rating

%rating, opinion, review

%Content, transaction, service, product, file, claim

%advisor, witness, second hand reputation


%internal vs agent-exposed: in the program, internal is how reputations, reviews, claims etc are stored and refers to [0,1] real
%agent-exposed is how the agent sees everything, typically [1,9] and discrete integers
%in the code \_i vs \_ae denotes which type a variable stores

%score: review value, claim value, reputation etc. referred generally


\iffalse
% CHAPTER ######################################################################
\chapter{State of the Art}\label{chap:sota}
% ##############################################################################
%State of the Art (SotA): Where is technology right now? How does it work? How does it relate to your work?

% sota:
% what a rep system is
% what is already there
% what can u do with them
% how much r they used
% notation of trust...

% sota merged with related work


\iffalse
%TEXT FRAGMENTS 
scale of these systems is so big
and agents are pseudo-anonymous, or do not have a grasp on each possible transaction partner because the network is so large

trust-based recommender or decision-support systems modeled people's trust in their acquaintances 
Then reputation systems model the societal reputation

% explain formattings like \emph for classnames and \texttt for modules variables and functions etc
\fi



% --------------------------------------------------------------
\section{Domains of Trust and Reputation Research}
% --------------------------------------------------------------

discuss fields like 
WSN
\gls{p2p} filesharing
MAS
mobile sensor systems
flying ad hoc networks
vehicular ad hoc networks
ecommerce
recommendation systems online stuff like imdb or hotel ratings

\fi



% CHAPTER ######################################################################
\chapter{Background and Related Work}\label{chap:related_work}
% ##############################################################################

%related work
% evaluation of related work
% shortly introduce rep systems
% discuss suitability
% decide to pick one or impl own

\lettrine{T}{his} chapter gives an overview of related trust and reputation research.
First, reputation and trust is introduced, including a quick outlook into usage within the \gls{iot} domain.
Then, published techniques for reputation system improvement are explored.
Most of these revolve around detecting and removing unfair ratings, or mitigating certain specific attack scenarios.
Finally, existing reputation system evaluation frameworks are taken into account.


\iffalse
% this is from my notes, some of our custom techniques too
there is a case to be made about applying improvement techniques depending on the environment of the reputation system. e.g.
rating aging makes sense only if behavior are known to change over time. if for sure constant (like machines which receive no updates whatsoever) then it has no point
frequency of feedback rating given after transaction.. means only something maybe if its not mandated, which in a fully machine network might as well just be
outlier filtering makes sense only if you know that significant (extreme) differences in transaction experiences are unlikely to the point that such claim becomes suspicious. Once highly subjective experiences come into play, it is wrong to say a nonmajority opinion is unfair. Or in a highly dynamic agent population, where behavior changes often and rapidly, extremity filtering is counterproductive since it introduces reputation lag.
differentiate based on how many ratings contributed to a final reputation score, i.e. confidence. like if a new entrant gets a default of 4, and another has 4 based on a 1000 ratings, this difference should be noticeable and agents should be able to make decisions based on that

outlier filtering based on something: statistical, entropy, dissimilarity function, similarity
dis/similarity is also a kind of stereotype-based TR, also collaborative filtering ties in here
normalizing rating range
weighing based on rater reputation
anonymous ratings
adding local trust
use random subsets to calculate reputation
Weighted Majority Algorithm 
simplifying rating space (some believe that +- or only + or - helps eliminate attacks)
feedback consistency-like solution -- in intersection, car estimates ETA, later rates its own ETA, and see if others rate the same ETA consistently -- inspired by one mentioned in \cite{azzedin_identifying_2010}

pair ETAs with uncertainty (generally: stake)
if I can say along with the claim how sure i am in the claim
or maybe once when I commission a car, I can say how accurate the sensor reading is, so this accuracy rating remains static for a long time
then if this uncertainty is larger, I have more room to manipulate the ETAs within plausible deniability, but on the other hand the other cars can say my claim is more worthless because I have a large uncertainty range..

also in this part: author reviews
\fi







% --------------------------------------------------------------
\section{Trust and Reputation}
% --------------------------------------------------------------

Researching trustability has been relevant in various domains for decades.
With network opening up and growing, more users and things who do not share history are interacting.
As new domains move to be integrated into networks and put online, more areas of life face the trustability problem.
Online trading or \gls{p2p} file sharing has been around for decades now, while autonomous driving, sensor systems or widespread connected embedded devices are more recent to face the issue of who to trust in a network.

Domains face different, but still similar challenges.
In \gls{p2p} file sharing, clients are constantly changing.
Some with malicious intent may want to poison the shared data in the network.\cite{kamvar_eigentrust_2003}
Any method to combat this must be completely decentralized as per \gls{p2p} protocols.
Online trading has also developed its methods to increase safety or at least a sense of it.
Shops and marketplaces extensively use user ratings, reviews and other methods of social proof to facilitate trust and aid the purchase decisions of customers.
The marketplaces themselves can serve as a central authority, and such systems are highly centralized since the marketplace is trusted by all participants\cite{josang_trust_2007}.

%Many approaches are inspired by the original field of trust and reputation: society.
%The idea is to see what methods people use to establish trust, make decisions, or navigate based on reputation, and imitate or be inspired from these.

We all have an innate understanding of trust, usually connected to reliability.
Trust can have two forms: reliability trust and decision trust\cite{josang_trust_2007,josang_survey_2017}.
Reliability trust is defined by Gambetta in \cite{gambetta_can_2000} and paraphrased by Jøsang et al. in \cite{josang_survey_2017} as
\begin{definition}[Reliability trust]
Trust is the subjective probability by which an individual, A, expects that another individual, B, performs a given action on which its welfare depends.
\end{definition}

As for decision trust, Jøsang et al. gives a definition in \cite{josang_survey_2017} as
\begin{definition}[Decision trust]
Trust is the extent to which one party is willing to depend on something or somebody in a given situation with a feeling of relative security, even though negative consequences are possible.
\end{definition}

Both definitions show that trust is a directed relational property between two individuals.
On the other hand, reputation is generally understood as a common and public judgment of an individual's reliability.
More broadly, Jøsang et al. gives the definition from the Concise Oxford dictionary as
\begin{definition}[Reputation]
Reputation is what is generally said or believed about a person’s or thing’s character or standing.
\end{definition}

Trust and reputation systems are designed to use these concepts in order to support decision making.
A decision can be for example who to transact with, which seller or service provider to choose, whether to accept data from a specific source or not, etc.
The participating actors that make decisions are are called agents, or in specific contexts also users, peers or nodes.
Agents can be people, machines or algorithms.

Trust management systems calculate trust relationships between individual agents.
Trust values are most commonly based on experience from previous interactions or third party opinions from other agents, usually called advisors or witnesses.

Reputation systems maintain a public reputation value for each agent, which can be used to assist agent decisions.
A trusted central authority is often in charge of maintaining global reputation values, but there are decentralized methods as well, for example in \gls{p2p} networks\cite{kamvar_eigentrust_2003,kurdi_honestpeer_2015,keshavarz_uastrustchain_2020}.

The focus of this thesis is reputation systems.
Because of this, the rest of this work gives less consideration for the components and special cases of trust-based systems.



% --------------------------------------------------------------
\subsection{Trust and Reputation in the Internet of Things}
% --------------------------------------------------------------

Due to the decentralized nature of \gls{iot}, many approaches opt for a more trust-focused approach.
MARINE is a trust model for vehicular ad-hoc networks (VANET) with two stages of evaluating trustworthiness\cite{ahmad_marine_2020}.
First, previous interactions and recommendations from neighboring vehicles are considered.
Then, data from the vehicle under evaluation is checked for
\begin{itemize}
\item information quality
\item vehicle’s message forwarding capability
\item opinions from neighbors
\end{itemize}
The evaluator vehicle drops data from the evaluated vehicle if any of the steps are failed.

Another approach for VANETs is found in \cite{primiero_simulation_2018}.
It proposes a trust method to mitigate a specific type of attack named Black Hole.
This attack consists of malicious nodes taking part in routing and dropping all data packets without forwarding them.

SecTrust is a trust-based method that provides secure communication and routing in the network, and isolates malicious or selfish nodes\cite{airehrour_trust-based_2017}.
Prior experience and other advisor node's opinions are both used to calculate trust values.

The work in \cite{ganeriwal_reputation-based_2008} introduces a reputation-based method for sensor networks.
Nodes privately store a reputation-like value from a subset of other nodes in the network, which is based on prior experiences of the nodes.
Together with opinions from other nodes, this forms the basis of trust decisions.
The system is distributed, so no central authority is required.

The authors of \cite{ahmed_trust_2019} conducted a survey among trust and reputation method in the field of \gls{iot}, also exploring basic concepts, taxonomy and commonly considered attacks.



% --------------------------------------------------------------
\section{Reputation Systems Improvement Techniques}\label{sec:related_improvements}
% --------------------------------------------------------------
% other name: accuracy, robustnesss, Trueness

Various papers propose methods to make reputation systems more resistant against attacks and biased or unfair ratings, and improve the accuracy of reputation calculation.
The work in \cite{zhang_detailed_2008} suggests four capabilities a complete improvement approach should have:
\begin{itemize}
\item \emph{majority}: still work if majority is unfair or malicious
\item \emph{flooding}: function under sudden onset of ratings in a short time
\item \emph{lack of experience}: still work when agents do not have any prior connections or private experience
\item \emph{varying}: agent behavior change should be recognized
\end{itemize}


\subsection{Categorization of Methods}

Whitby et al. group methods to identify and remove unfair ratings into two categories: \emph{endogenous and exogenous} approaches\cite{whitby_filtering_2014}.
Endogenous refers to methods where only the rating values are analyzed for statistical anomalies.
Exogenous methods consider factors other than the ratings themselves for detecting unfair ratings.
Such external factors are for example the reputation of the rater, in which case the assumption is that low-reputation users are more likely to give unfair ratings.
%Endogenous could be expanded or generalized to say only the ratings, reviews, feedback, including their value and accompanying text, any data and metadata are analyzed.

\emph{Reactive-proactive} categorization is introduced in \cite{thakur_reputation_2019}.
A method is reactive if it identifies existing, already published unfair feedback.
Opposed to this, a proactive methods proposes incentives for agents to encourage them to leave fair feedback, so that unfair feedback is prevented before given.

The authors of \cite{zhang_detailed_2008} use two other categories apart from endogenous-exogenous: \emph{public-private} and \emph{global-local}.
These are adopted specifically for e-commerce-type scenarios where local trust (past experiences) and local advisor opinions both play a role.

\emph{Public and private} are differentiated along how advisor opinions are judged for trustworthiness.
In private methods, agents evaluate an advisor's opinion based on whether past opinions received by the same advisor turned out to be correct or not.
Data for evaluation is limited to the specific advisor's interactions with the specific agent in the past.
In public methods, agents judge advisors based on the advisor's ratings across the whole system in the past.
Here, all previous opinions of the advisor and their subsequent results are taken into account, not just those which were provided for the agent currently doing the evaluation.

\emph{Local} refers to methods where looking for received unfair ratings of an agent is based on all the ratings that agent received, but not other agents.
For example, if seller typically gets reviews around 4 or 5 out of 5, a 1-rating raises suspicion.
In a sense, opinions are isolated between agents.
In \emph{global methods}, advisor or rater trustworthiness is judged using ratings on all the agents in the system.

Although defined for an e-commerce advisor-opinion scenario, these categories can be understood in a more general sense.
Sellers can be generalized as agents providing content, be it data, service, product or sensor reading, etc.
They receive a rating in turn.
Vice versa, buyers are receiving the content and providing the rating.
Advisor refers to any third party contributing to the final aggregated reputation of a potential transaction partner.
Opinion is any rating, review or feedback in a more broader sense.

%The public-private and local-global distinction can be understood as two sides of the transaction. In public-private, the rater is judging, 

%accuracy (better function) vs attack countering
%they can overlap

%categories based on domain 
%mentioned, but not the main driver of categorization here, because looking for general ideas to implement in IoT etc as cross domain
%better if domain dependent or independent

%based on general idea

%other category: what is rated
%agents 
%content
%links
%could call this rating dimension aka who/what is rated exactly
%can represent as graph (vertices, edges, directed boxes as content)
%discuss special cases, e.g. in e-com, rating the very same physical products between different sellers, an extra rating dimension is needed for service quality (packaging, mayb its white/gray label, shipping time, customer service interaction's quality etc... this is different than the product rating and also diff from the agent rating itself
%also accuracy rating


\subsection{Statistical Filtering Methods}

Many reputation systems are based on statistical methods, and statistical analysis can be used to further improve reputation systems.
Whitby et al. proposes a filtering mechanism for unfair opinions for the Beta Reputation System (BRS)\cite{whitby_filtering_2014}.
This method extends BRS, which is a Bayesian reputation system based on the beta distribution.
The authors argue that unfair and fair ratings have different statistical patterns, mainly seen as differences to majority opinion.
Consequently, it is possible to identify and filter unfair ratings with statistical methods.
The basis of filtering like this is the assumption that the majority of the users are honest and accurate.

TRAVOS is also a Bayesian trust and reputation method based on the beta probability distribution\cite{teacy_travos_2006}.
It uses both experience-based local trust and global reputation, depending on whether past experience with a particular transaction partner is available or not.
If a potential transaction partner is not already well known, the system relies on global reputation reported by third party advisors.
Deception from advisors is countered by evaluating the perceived accuracy of their past opinions.
Specifically, first a probability is calculated for how likely it is the third party advisor gives an accurate opinion.
This is done on the basis of previous opinions given, and the eventual outcomes of those transactions.
Then as a second step, the opinions likely to be inaccurate are altered so as to have a smaller impact on the final calculated reputation.
This is done through decreasing the parameters of the distribution which represents the advisor's opinion. An opinion modified in this manner will influence the expected value of the final reputation distribution to a lesser degree.

The Beta Reputation System and TRAVOS are very similar.
Both are Bayesian statistical methods using the beta probability distribution.
However, BRS does not differentiate between direct observations and advisor recommendations.
Additionally, the two methods handle inaccurate (unfair or malicious) ratings differently.
TRAVOS is an exogenous approach, scrutinizing each individual advisor based on the perceived accuracy of their past opinions.
BRS is an endogenous approach, taking a single rating and judging it based on how far it is from the mainstream opinion.

%There is the question of what to do with opinions caught in the filter. Completely disregard, lower weight, or temporarily disregard...

The Dirichlet Reputation System is worth mentioning as a multinomial generalization of the Beta Reputation System\cite{josang_dirichlet_2007}.
Using the Dirichlet distribution instead of the beta distribution allows more than two discrete rating levels.
It is a method well grounded in Bayesian statistics, and is built around concepts similar to that of the BRS.
Specifically, aging of ratings is also present.
A similar extension for filtering non-majority outlier opinions like in the BRS was not found for the Dirichlet Reputation System.

The Weighted Majority Algorithm introduced in \cite{yu_detecting_2003} gives advisor agents a weight to aggregate their opinions with.
If an opinion turns out to be incorrect based on the outcome of the involved transaction, the advisor's weight is decreased.
Constantly wrong advisors are thus filtered out eventually.

%The authors of \cite{zhang_personalized_2006} and \cite{zhang_detailed_2008} take a survey of previous improvement methods, introduce a new method (personalized approach) and compare them, including experimental measurements. The methods compared are the Beta Reputation System, TRAVOS, Personalized Approach, Bayesian Network Approach and Weighted Majority Algorithm.
%\cite{zhang_detailed_2008} does an ad-hoc comparison of accuracy improvement / unfairness filtering methods.


\subsection{Methods Based on Similarity}

The authors of \cite{zhang_personalized_2006} list four capabilities of ideal unfair rating handling methods.
These are
\begin{itemize}
\item handle unfairly high ratings
\item handle unfairly low ratings
\item deal with behavior changes of agents
\item handle that agents may have genuinely different opinions on service providers
\end{itemize}
The Personalized Approach is introduced to fulfill the last criteria\cite{zhang_personalized_2006}.
Apart from each agent having a public global reputation, every one of them stores private opinions of agents they have previously interacted with.
When an agent looks for opinions on a potential partner, advisor agents provide their opinion.
To judge the reliability of these opinions, the asking agent compares its private opinion list with that of the advisor.
If they have formed many similar opinions on the same third party agents, their preference is assumed to be similar and the opinion from that advisor is assumed more relevant.

The work in \cite{iltaf_mechanism_2013} proposes a dissimilarity function for detecting deviations in a recommendation set.
The method assumes that dishonest recommendations are inconsistent with other recommendations and they also have a low occurrence rate.
A similar dissimilarity function is also introduced for mobile ad-hoc networks in \cite{zakirullah_detection_2014}.

The authors of \cite{dellarocas_immunizing_2000} introduce two trust or reputation system extensions to combat unfair ratings in e-commerce settings:
\begin{itemize}
\item controlled anonymity to avoid unfairly low ratings and negative discrimination
\item cluster filtering to reduce the effect of unfairly high ratings and positive discrimination
\end{itemize}
Cluster filtering uses collaborative filtering techniques to identify the nearest neighbors of agents, based on similarities in commonly rated other agents.
The goal is to identify conspirational collectives who give unfairly positive ratings to increase a specific provider's reputation.

The method proposed in \cite{zupancic_qade_2015} looks for similar agents and consider these as trustworthy sources of information for each other.
Agents compare their trust values with trust values of similar agents.


\subsection{Other Methods}

The authors of \cite{rezvani_randomized_2020} randomly select samples of all reviews to calculate global reputation in an e-commerce context.

A complex five-step process is adopted in \cite{baby_secure_2014} to make the reputation system more secure against unfair ratings.
The steps are
\begin{itemize}
\item Evaluation-based filtering
\item Time-domain unfair rating detection
\item Suspicious user correlation analysis
\item Trust analysis based on Dempster-Shafer theory
\item Malicious user identification and reputation recovery
\end{itemize}

%\cite{zhang_personalized_2006} also does survey (short)
%\cite{ngo_survey_2007} also surveys methods against unfair ratings. Considered approaches are
%filtering based on majority opinion (Whitby et al), 
%entropy-based filtering, 
%reputation trees, (agents reputation is their rating's weight)
%controlled anonymity and agent clustering,
%clustering ratings and checking IP address, 
%using trust rating in MANETs and 
%WMA and belief function.

The feedback consistency method is described in \cite{azzedin_identifying_2010}.
Both of the transaction participants leave ratings on the transaction.
The ratings are considered consistent if they evaluate the transaction with the same outcome, whether successful or not.
An inconsistency is assumed to mean one of the transaction partners is not honest.
When an agent reaches a certain threshold with its ratio of inconsistent vs. consistent feedback, the agent is suspected of leaving false feedback, and feedback from such agents are discounted.

The Context-Aware Approach aims to improve unfair rating detection by analyzing selected factors of the environment the detection algorithm is deployed in, and selecting the most suitable detection approach from a set of methods\cite{wan_context-aware_2012}.

Some approaches provide incentives for honest behavior to prevent or mitigate unfair ratings\cite{thakur_reputation_2019, jurca_minimum_2006}.

The method proposed in \cite{liu_reputation_2015} recovers the temporary damage of sellers who were given a lower rating by a buyer by mistake, and the buyer later willingly corrected this.
The seller's reputation is temporarily inflated to revert reputation damage.

First hand trust observations and global reputation are combined in some works\cite{boudec_robust_2003,huynh_handling_2005}.
The method from \cite{boudec_robust_2003} is developed for the mobile ad-hoc network domain.
Every node maintains a first-hand reputation (trust) and a second-hand (global) reputation of others.
First-hand reputation is exchanged by nodes occasionally, and data is combined to reach a better accuracy.
The system employs aging and occasional reputation re-evaluation to enable reputation redemption.

Interdisciplinary approaches have also been introduced to improve reputation.
Machine learning can monitor statistical properties or agent behavior to filter the reputation system\cite{khoshkbarchi_coping_2017,wang_bayesian_2003,yazidi_solving_2017}.
Combining subjective recommendation systems with reputation systems to achieve better final advice is proposed in \cite{hutchison_combining_2013}.
Rating scale normalization comes from collaborative filtering systems\cite{margaris_improving_2017,margaris_improving_2018}.
Each agent's typical rating scale is determined and the different scales are normalized into a common scale.



% this part obv not here, rather in approach
%explicit inclusion of private experience as a system component
%possibility of maintaining a trust network and getting recommendations (opinions) from them as advisors

%both of these are done in a system primarily used by men. e.g. in e-commerce, once the reliability and quality of a provider is known through first-hand experience, reviews and ratings are not scrutinized as they would be by first-time buyers. Social proof mechanisms like getting recommendations and opinions from friends or trusted contacts is also very common without it needing to be part of any reputation system. informally there

%This is different in reputation systems made for and used by algorithmic agents. whether decisions are made by other machine agents or the system (algorithm) itself like in a traffic management scenario, it will only ever consider factors it was programmed to. There is a case for explicitly integrating these human mechanisms or similar methods inspired by them. The potential upside is better reputation estimate accuracy, or at the least better decision outcomes. On the other hand, the increased complexity provides new attack vectors and thus potentially enables new exploits.

%Mechanisms like a list of trusted "friends," trust networks, third party advisors providing private opinions come from a trust-centric approach to the decision making problem. Three separate categories give themselves from this distinction: pure reputation systems, reputation systems infused with trust components and pure trust systems. Only the first two is of interest within the scope of this thesis.

%With this, the categorization of local-global and public-private is also easier to define in a broader context.


%Accuracy of ratings is a crucial part of any reputation system. This is true whether transactions or directly the users are rated. 
%Many attacks exploit exactly this vulnerable point of TRs. Slandering, promoting and other attack strategies, see attacks chapter...

%Interpreting accuracy of ratings is difficult in systems where people rate based on their subjective experiences, or where there is a degree of uncertainty. E.g. e-commerce rep systems
%Since ratings in real-life situations are not accurately computable
%analogous term is honesty of users. some researchers (cite?) use a scale based on honesty to categorize users from purely malicious to purely honest. In this case this refers to given feedback, as other dimensions of "inhonesty" is also possible, like malice in transaction content. There was a paper that categorized malice into these two ways, cite mayb. Also this discussion mayb not here.

%Accuracy's further interpretation is a sort of "trueness" value. This implies that an objectively true and absolutely accurate rating of a user's experience exists.
%This is then different from the feedback the user gives into the system.
%The user may or may not be aware of this "true rating." If not aware, maybe bias, subjectively very unpleasant experience. Some call this inherent bias. E.g. e-com the product is perfect but shipping time was bad and gives a 1-star rating. 
%(There is a philosophical aspect to this, as this is saying "you experienced this a 1/5 but we say it really was a 4/5" and what right or objective measure exists to permit this.)
%If the user is aware of the "true quality," giving a doctored rating out of malicious intent is still a possibility. 
%In any case, the result is that the given rating differs from a hidden, "objectively true" rating. This situation is called an "unfair" rating by Josang.

%(continuation of above) machine populated reputation systems where ratings are given by algorithms evaluating data, are more fitting to calculations of numerical accuracy




% --------------------------------------------------------------
\section{Evaluation and Comparison}\label{sec:evaluation_systems}
% --------------------------------------------------------------

\begin{table}[tbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Name}        & \textbf{Domain} \\ \midrule
Agate\&etal18\cite{agate_platform_2018,agate_reputation_2020} & distributed systems \\
ART\cite{fullam_specication_2005} & \glspl{mas} (art appraisal) \\
ATB\cite{jelenc_decision_2013} & \glspl{mas} \\
C\&E15\cite{chandrasekaran_toward_2015} & generic \\
DART\cite{salehi-abari_dart_2012} & \glspl{mas} \\
EstarMom\cite{nguyen_estarmom_2014} & \glspl{mas} \\
Euphemus\cite{adamopoulou_simulation_2014} & online market \\
Irissappane\&etal12\cite{irissappane_towards_2012} & marketplace \\
Janiszewski17\cite{janiszewski_towards_2017} & generic, theoretical \\
Liang\&Shi08\cite{liang_analysis_2008} & \gls{p2p} \\
Macau\cite{hazard_macau_2013} & \glspl{mas} \\
NEVER\cite{fernandez-gago_network-aware_2013} & open networks \\
QTM-P2P-Sim\cite{west_evaluation_2010} & \gls{p2p} \\
Schlosser\&etal04\cite{schlosser_comparing_2004} & \glspl{mas} \\
SIFT\cite{suryanarayana_sift_2007} & decentralized open networks \\
TO-Sim\cite{zhang_simulating_2007} & \gls{p2p} \\
TREET\cite{kerr_treet_2010} & \glspl{mas} (marketplace) \\
TRM-EAT\cite{janiszewski_trm-eat_2020} & generic \\
TRMSim-WSN\cite{marmol_trmsim-wsn_2009} & Wireless Sensor Networks \\
Youssef\&etal17\cite{youssef_towards_2017} & \glspl{mas} \\
\bottomrule
\end{tabular}
\caption{
	Reputation evaluation frameworks in the literature, and the domains they focus on.
	Multi-agent systems can encompass a broad range of environments, they can be used at least partially generally.
}
\label{tab:literature_evaluation_frameworks}
\end{table}


Literature has seen a number of publications that evaluate reputation methods or create freestanding evaluation frameworks for this purpose.
Table~\ref{tab:literature_evaluation_frameworks} lists these attempts, along with their domains.
This section describes the most notable evaluation and comparison approaches.

The ART testbed is one of the earliest comprehensive evaluation frameworks published for comparing trust and reputation systems\cite{fullam_specication_2005}.
It simulates a \acrfull{mas} environment of independent agents within an artwork marketplace.
Multi-agent systems refer to any kind of collaboration environments, where intelligent independent agents operate in order to reach their goals.
Agents in MAS environments typically have to collaborate with each other in order to reach their goals.
An agent's goal in the ART testbed is to collect the most utility by the end of the simulation, which serves as the basis of comparison between different agent approaches.
Thus, results from ART reflect each agent's capability in navigating the marketplace scenario, rather than the overall effect of the reputation system.
ART also does not directly support testing to what degree reputation methods can discourage or remove malicious intent.

TREET is a follow-up to the ART testbed\cite{kerr_treet_2010}.
Its goal is to focus more on evaluation of trust and reputation systems and their resistance to security attacks within an e-commerce setting.
TREET can simulate both centralized and decentralized reputation systems, supports collusion attacks and extension with custom attack methods.
The testbed's drawback lies in its commitment to the e-commerce domain.
Also, TREET's metrics evaluate based on the success agents see as a result of their decision making, and not the quality of reputation or trust values.

The Alpha Testbed (ATB) focuses on the agent's decision making mechanisms\cite{jelenc_decision_2013}.
The authors argue that the way agents decide who they transact with influences evaluations of reputation systems.
ATB supports changing the decision mechanisms of agents independently of the reputation or trust model, thus separating the two.
Domain independence is another advantage of this framework.
Custom metrics are possible to implement, although researchers have pointed out that system-level metrics that evaluate system-wide performance are not supported\cite{adamopoulou_simulation_2014}.
ATB's authors mention its inability to accommodate trust models that represent information substantially different from formats used in ATB, or when opinions are computed or shared using special protocols.
Additionally, ATB only simulates one single agent in full, called the alpha agent.
All other agents are a represented by mocks and do not actually use the simulated trust system.
This is a clear disadvantage when a trust or reputation system revolves around social interactions or virtual organization among agents.

The QTM or P2P-Sim simulator focuses on trust and reputation systems deployed in \gls{p2p} file sharing networks.
QTM is a flexible system that supports custom reputation methods and attack scenarios.
A trace file serves as input, which lists in advance which agents request which files, determining the simulation's transactions.
Evaluation is based on a clearly defined metric called hit rate, calculated as
\begin{equation}
\mathrm{hit~rate}=\frac{\text{\# valid files received by good users}}{\text{\# transactions attempted by good users}}
\end{equation}
Implementing custom metrics is not supported by default.
As the QTM authors say, shared files can either be valid or invalid, and binary feedback is expected after each file sharing transaction.
Supporting only binary transaction outcomes and feedback values is QTM's major limitation.
Tested trust or reputation models also need to contain a selection procedure for peer agents to decide who they download their requested files from.
This is not always part of models.

TRMSim-WSN is a simulator for trust and reputation systems specifically tailored for wireless sensor networks\cite{marmol_trmsim-wsn_2009}.
Highly domain-specific features like routing, energy consumption, radio range, battery and source/sink nodes limit this simulator's cross-domain applicability.

The works in \cite{agate_platform_2018,agate_reputation_2020} (Agate\&etal18) introduce an evaluation system for distributed reputation algorithms.
This relatively recent (2018-2020) framework aims to support reputation management system design by testing robustness against security attacks.
The framework stays domain-independent by disregarding communication protocols between agents and the reputation system as well, while enabling custom reputation calculation methods and agent behavior.
Agents are also allowed to implement their own decision making for selecting transaction partners.

The framework Youssef\&etal17 presents a \gls{mas} scenario in which students with different skills must complete their homework\cite{youssef_towards_2017}.
The work Janiszewski17 introduces a set of theoretical metrics that can be used to evaluate different reputation systems\cite{janiszewski_towards_2017}.



%% --------------------------------------------------------------
%\section{Attacks and Exploits}\label{sec:attacks}
%% --------------------------------------------------------------

%Seller sells most products with high quality, but some with low quality for benefit. The honest bad reputation report from victim may be submerged, or be mistakenly regarded as unfair rating. \cite{ping_xu_rating_2005}

%\cite{yu_detecting_2003} has the four static in the graph..
%static:
%"pessimistic" rater (rates in the lower range) bad mouthing
%"optimistic" rater (rates in the higher range) ballot stuffing
%inverted rating (rates good as bad and vice versa)
%random rater (rates random all the time)




% CHAPTER ######################################################################
\chapter{Approach}\label{chap:approach}
% ##############################################################################
%Materials, Methods, Models, Algorithms, or Tools: Explain what the reader needs to understand in order to understand your approach.
%    Which existing methods and tools will you use to approach the problem?
%    What is your system model? What are your assumptions? 

%Approach: Explain your concept to solve the problem.
%    Intuition first: Give a short, informal description of your idea using a simple diagram and/or a simple example.
%    Architecture/Model: Fully describe your approach on a conceptual level without giving details that belong to only one possible implementation.
%    Example: Write “We need a hash function with the following properties.” NOT “We need/use SHA-256”

% approach can have 3 types of content:
%    selecting what you used from a larger group of many ideas
%    changing existing ideas, why and how
%    completely new ideas, while saying xyz requirements r not fulfilled by previous ones


%somewhere in this chapter, discuss how in ecom or other domains its a problem that not many ratings are given, while in fully machine populated it can be mandated to leave a review. then again its a question of who's in sensing distance e.g. in the intersection management case

%why there should be claims and reviews scenarios...
%should be scenarios to sim diff behaviors of agents
%why do u think claims and models can model a reputation system
%the idea behind creating this model

%approach: how can u evaluate different rep techniques
\lettrine{T}{his} chapter describes how reputation environments are modeled to allow comparison of improvement techniques.
First, the model elements are discussed.
Then, configurable parts like reputation and improvement methods, and behaviors of participating agents are covered.
Finally, concepts of the evaluation framework used for simulations are introduced.

Table~\ref{tab:model_notations} provides a reference for notations introduced throughout this chapter.

\begin{table}[tbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Notation}        & \textbf{Meaning} \\ \midrule
$A$ & all agents of a scenario \\
$a_m$ & the $m^{th}$ agent, $a_m\in A$ \\
$C^r_{i,m}$ & the claim with index $i$, made by agent $a_m$ in round $r$\\
$Q_{meas,m,i}$ & quality of claim $C_i$ as measured (seen) by agent $a_m$ \\
$Q_{true,i}$ & ground truth of claim $C_i$ \\
$R_{m,i}$ & a review of claim $C_i$, the review is by agent $a_m$ \\
$R_{author,m,i}$ & the author review of $C_{i,m}$, made by agent $a_m$ \\
\bottomrule
\end{tabular}
\caption{
    Reference of notations used in the introduced model.
    When the information carried by a subscript is not relevant in the mentioned context, that is omitted from the notation for clarity.
}
\label{tab:model_notations}
\end{table}



% --------------------------------------------------------------
\section{Transactions and Content}\label{sec:approach_content}
% --------------------------------------------------------------
In the following of the thesis we assume that a reputation system consists of a set of \emph{agents} $ A = \{a_1, a_2, ... a_n\} $, which engage in transactions with each other over time.
Transactions can be data transfer, a purchase, service usage, etc.
One of the transaction partners fills the role of a \emph{provider}, while the other is a \emph{consumer}.

Transactions involve the delivery or exchange of something, like products, service provision or any kind of data.
Concrete examples are a book, a hotel room, files shared on a \acrlong{p2p} network, an online comment, a contractual agreement.
It can also be sensor data or an algorithmic result like a self-driving car's estimated time of arrival to the next intersection.

We refer to these objects of exchange as \emph{content} from here on.
Because the model aims to stay domain-independent, we elected to 
\begin{itemize}
\item not explicitly define the specifics of transactions
\item not model the content
\item not model content-exchange processes or protocols
\end{itemize}



% --------------------------------------------------------------
\section{Claims}\label{sec:approach_claims}
% --------------------------------------------------------------
%In reputation systems, transacting agents may leave a public feedback of their transaction partner or of the received content, or they may just record their opinion and provide it later upon request.

Before agents commit to a transaction, they have a preliminary understanding of what content exchange would take place.
Claims
\footnote{The claim name comes from its old use. Formerly, the author review was called claim value. It was said the agent "claims that the quality of its content is \emph{xyz}"}
model the promoses of content providers.
Claims are denoted as $ C_{i,m} $, where $i$ is the claim's identifier index, and agent $a_m$ is the claim's author.
Conceptually, a claim is made up by the following components.
\begin{enumerate}
	\item representation of the provided content's quality, named \gls{gr_truth} or claim quality
	\item the author agent that made the claim
	\item an \gls{author_review}
	\item the time from which the claim is available (published)
\end{enumerate}

Claims do not explicitly contain anything about the content they represent, because as discussed previously, content is not modeled in order to stay domain-independent.
Content is only implied with the claim, and only its quality is represented.
More broadly, claims do not contain anything about what the context of the reputation scheme is.
The following is all assumed:
\begin{enumerate}
	\item some kind of content behind the claim
	\item a way of offering or distributing said content
	\item an environment in which the content is produced and consumed
\end{enumerate}

Claiming (Figure~\ref{fig:claim_rate_process}, above) is the process of making (publishing) new claims.
It start with a new claim $C_{i,m}$ without any reviews, containing a \gls{gr_truth}.
The claimer agent $a_m$ measures this hidden quality and distorts it, resulting in a value representing the claimer's opinion of his own claim.
This is attached as an \gls{author_review} $R_{author,m,i}$ to the claim to be published.
Subsequent sections in this chapter discuss the contepts of ground truth, distortion and reviews.


\subsection{Quality Valuation}
Quality valuations or estimations model a quantitative valuation of the quality or goodness of the content.
What this means can differ between domains.
Table~\ref{tab:content_examples} lists examples for contents and their possible valuations.
For example, in an online marketplace of entertainment products, the valuation can be some approximation of the entertainment value the content provides.
In the case of a file-sharing context, it could indicate whether the shared files that form the content are intact, and what is provided is also what was promised.

Three kinds of valuation are modeled, distinguished by the source of the valuation. These are:
\begin{itemize}
	\item \gls{gr_truth} (or claim quality)
	\item author's review
	\item regular review
\end{itemize}

\begin{table}[tbp]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Context}        & \textbf{Example Content} & \textbf{Example Meaning of Valuation} \\ \midrule
File sharing       & Legally distributed music & File is what was promised \\
Marketplace        & Books & Enjoyment \\
Sensor data     & Estimated time of arrival & Accuracy of estimate \\
Online discussion   & Social media post & Popularity, agreed or not \\
\bottomrule
\end{tabular}
\caption{
	Example contents and their possible quality valuations within various contexts.
	Some valuations are more subjective, others can be accurately judged.
}
\label{tab:content_examples}
\end{table}


\subsection{Ground Truth}\label{sec:approach_gr_truth}
The \gls{gr_truth}, or alternatively called claim quality, denoted by $ Q_{true,i} $ is meant to model the objectively true but hidden and not knowable quality of a content behind the claim $ C_{i,m} $.
This assumes that there is a way to calculate or judge a content's universal quality with the chosen valuation.
In the case of the autonomous vehicle's estimated time of arrival, the ground truth represents the actual physically measurable length of time the vehicle needed for arrival.
Of course, accurate and objective judgment is often not possible.
For example, the enjoyment of books, or the validity of a social media post is hardly universally estimable.

Still, ground truth is a useful tool for modeling reputation schemes.
A baseline truth is often needed for evaluating the performance of the reputation scheme, or in the case of this work, improvement techniques.
Additionally, such a claim quality can be used as a starting point when modeling how other agents see the content.
This method is called measuring claims and is discussed in Section~\ref{sec:approach_measure_claim}.

Ground truth is an inherent quality in every claim.
When an agent claims, the new claim will contain a ground truth value that is not controlled or influenced by the claimer at all.
In this model, a ground truth is seen as a kind of circumstance, one that would only show as time progresses.
For example, when the autonomous vehicle makes an estimate for a time of arrival, the real and accurate time taken to arrive is not yet known.
For modeling purposes it is assumed that it will be known, and it was in fact generated as ground truth at the time the claim was made.

Finally, ground truths are hidden from agents, and they have no way of directly accessing this valuation.
The closest estimation of the ground truth value that agents can obtain is through the above-mentioned measuring process, which is a model of trying out the content and forming an opinion of it.


\subsection{Author's Review}\label{sec:approach_author_review}
An \gls{author_review} $ R_{author,m,i} $ is a valuation modeling what the agent $a_m$ who is author of claim $C_{i,m}$ says the quality of the content is.
It is a self-appraisal, a declaration of what the claimer thinks its claim is worth.
Modeling the author's opinion is a somewhat unusual approach, since this is usually not explicitly part of reputation systems.
The benefit of having author reviews is that reputation calculation methods or agent decision making can use it.
When this is not needed, other parts of the model would simply disregard author's reviews.

Author reviews can be interpreted in multiple ways.
When the reputation scheme uses an explicit value which agents publish along their claims, it is exactly that, an explicit self-assessment of provided content.
Otherwise, it can imply an unspoken, but still advertised positioning of the claim.
This can be a marketing message for example, or an expectation that a specific director's movies are usually good.
These give no explicit valuations tied to contents, but some kind of positioning can still be implied from the image, history, expectations, etc.


\subsection{Measuring Claims}\label{sec:approach_measure_claim}
Measuring claims is the term used for when an agent assesses the quality of a claim.
This models when a consumer upon receiving a content, tries it out and leaves feedback based on first-hand experience.
Additionally, an agent can also measure its own claim.
%Measuring a claim is the representation of trying out a product or content and making an opinion of what its quality could be.

The measured quality $Q_{meas,m,i}$ denotes agent $a_m$'s assessment of claim $C_{i,n}$.
With the model's assumption on the existence of a ground truth, measuring a claim can be seen as an agent trying to guess this hidden quality valuation.
Agents are modeled here with various levels of expertise, which affects how close or how reliably they can measure the ground truth.
Know-how, experience, expertise or inside knowledge are among the possible factors playing a role.
The model's implementation uses \acrlong{ctai} for this, which is a parameter of agents representing the maximum inaccuracy they can make when measuring.

As discussed, making a new claim has two main stages: measuring the ground truth and distorting.
The measurement serves as input for distortion.
Distortion models conscious actions, honesty or willing manipulation.
Measuring claims is also a sort of distortion, albeit not under the control of the agent.
While distortion is \emph{intent}, measurement represents \emph{competence}.



% --------------------------------------------------------------
\section{Regular Reviews}\label{sec:approach_reviews}
% --------------------------------------------------------------
Regular reviews (or simply reviews) model the feedback on products from agents that are on the consumer end of transactions.
A review $R_{m,i}$ is the opinion of agent $a_m$ on claim $C_{i,n}$, where $m \neq n$.
Every review implies that a transaction took place, and content was exchanged.
The agent $a_m$ who leaves the review is in the consumer role, and is called \emph{rater}.

A fundamental assumption of the model is that raters leave feedback on the content, not the content's provider.
This means that the subject of reviews are claims, not the claimers.
A rater leaving a review on a claim represents what that rater thinks the content quality is.
Another interpretation is that a review is what the rater thinks the rated claim's author review should have been.
In essence, what the rater would publish as author's review, were that rater the author of that claim.
Both interpretations have the same outcome.



% --------------------------------------------------------------
\section{Reputation Calculation}\label{sec:approach_reputation_calculation}
% --------------------------------------------------------------
Two reputation calculation methods were designed for evaluation, both of them based on relatively simple averaging.
One drawback of using custom methods is they are not directly comparable with any other related reputation system.
Another issue is both methods assign reputation to claiming agents only.
Consumers who only rate and never claim, do not get a reputation.

The two methods compensate with the benefits of simplicity.
Unlike with more complex methods, it is possible to easily follow what happens.
Averaging is also a very common-sense approach.
Many people would expect something like this when seeing a score-based reputation system, although this is rarely the case.
Finally, both methods can support weights, one of the improvement techniques used.

\subsection{Difference of Author Reviews and Regular Reviews}\label{sec:approac_repcalc_basedondifference}
This method makes use of the unique author's review system.
It exploits that both the author and consumers make reviews of claims.
Assume an agent $a_m$ has made a series of claims $\delta=\{C_{i,m}, C_{i+1,m},...C_{i+N-1,m}\}$, that is $N$ number of claims in total.
To get the reputation of agent $a_m$,, each of its published claims $\delta$ has its reviews averaged, and the difference between this average and the author's review is calculated (\ref{eq:basedondiffrep1}). Once ready for all of an agent's claims, the differences themselves are averaged as in (\ref{eq:basedondiffrep2}).
\begin{equation} \label{eq:basedondiffrep1}
\begin{aligned}
\mathrm{difference}(C_{i,m})&=\left|\frac{\sum_{n \in A} R_{n,i}}{\text{number of reviews }R_{n,i}}-R_{author,m,i}\right| \\
\text{where}&~ n\neq m
\end{aligned}
\end{equation}
\begin{equation} \label{eq:basedondiffrep2}
\mathrm{average~difference}(a_m)=\frac{\sum_{i\in\delta}\mathrm{difference}(C_{i,m})}{\text{number of claims in }\delta}
\end{equation}

The maximum possible difference is a property of the reputation scheme, known as the difference between the lowest and highest possible review valuation.

Finally, the agent's reputation is calculated based on the percentual relationship of the average of differences and the theoretical maximum difference (\ref{eq:basedondiffrep3}).
\begin{equation} \label{eq:basedondiffrep3}
\begin{aligned}
\tau &=\frac{(\mathrm{max~difference})-\mathrm{average~difference}(a_m)}{\mathrm{max~difference}} \\
\mathrm{reputation}(a_m)&=\tau*(\text{reputation range})+\text{min reputation} \\
\text{where}&~ \mathrm{max~difference}=(\text{max reputation})-(\text{min reputation}) \\
&~ \text{reputation range}=(\text{max reputation})-(\text{min reputation})
\end{aligned}
\end{equation}
% diff_percent = (max_difference - avg_diff) / max_difference
% reputation = min_reputation + (max_reputation - min_reputation) * diff_percent

If an agent's author reviews of its own claims would perfectly align with the reviews they received, the agent's reputation becomes the maximum possible.
Similarly, if the difference is at the highest possible, e.g. when an agent's author reviews are all at the maximum rating and all reviews are at minimum, the reputation becomes the lowest possible.
In between it is straight proportional.

Basically honesty of claimers is rewarded, while dishonesty is punished through reputation.
Reputation are given based on how close agents rate their own claims compared to the average opinion.
Another way to explain it, is that regular raters approximate the claim's ground truth.
Here, an agent's reputation comes from how close that agent valued its own claims compared to the claims ground truths as approximated by reviewers.
In the end, an agent's reputation informs others how reliable that agent's author reviews are.


\subsection{Simple Review Average}
Here, reputation is the average of received reviews.
The method simply collects every review on all claims an agent has.
The average of these received reviews becomes the agent's reputation.
Collecting and averaging is repeated separately for all agents.
As such, the raputation of agent $a_m$ is given as
\begin{equation}
\begin{aligned}
\mathrm{reputation}(a_{m})&=\frac{\sum_{i \in \omega} R_{n,i}}{\text{number of reviews }R_{n,i}} \\
\text{where}&~ n\neq m \\
&~ \omega=\text{all claims by agent }a_m
\end{aligned}
\end{equation}

The specialty in simple averaging of reviews is in the different interpretation of reviews.
Section~\ref{sec:approach_reviews} discussed previously how reviews are meant to give feedback on claims, not on the claimer agents.
Simple review averaging turns this around and makes reviews behave as rating the claimers themselves.

This is best seen through an example.
Let the rating range be [1,9], i.e. for all reviews $\forall R: 1\leq R \leq 9$ and all quality variations $\forall Q: 1\leq R \leq 9$.
An honest claimer $a_m$ publishes a claim $C_{i,m}$, which has a low ground truth of $Q_{true,i}=2.5$.
Accordingly, let the claimer's author review of the claim is also low, because the claimer is an honest agent.
Let the author review be $R_{author,m,i}=2$.
If some other agents $a_n, a_o, a_p$ then rate this claim, and they rate based on the quality of the content represented by it, they would leave mostly low reviews.
Their reviews could be
\begin{equation}
\begin{aligned}
R_{n,i} &= 2 \\
R_{o,i} &= 1 \\
R_{p,i} &= 3 \\
\end{aligned}
\end{equation}
Assuming $a_m$ has no other claims and no other agents have given ratings apart from the three above, the reputation of the claimer $a_m$ becomes
\begin{equation}
\mathrm{reputation}(a_{m})=\frac{R_{n,i}+R_{o,i}+R_{p,i}}{\text{number of reviews}} = \frac{2+1+3}{3} = 2
\end{equation}
Even though the claimer was honest in declaring its own content as of low quality, the resulting reputation is still low.
Here, the reputation signals that the claimer provides content that is low in quality.

The other case is when raters review the claimer itself, how good their transaction experience was, whether it went according to expectation, etc.
So raters consuming a content behind the above claim fully expect it to be of low quality, because the author's review also warned them.
Assuming the transaction goes well, raters leave a good (high) review.
This results in a higher reputation for the claimer.
In this case, reputation signals if claimers are honest.

As seen, Simple Review Averaging changes what reviews and reputation mean away from their default intended meaning.
For this reason, the use of Simple Review Averaging was mostly neglected in favor of the other reputation calculation method described in Section~\ref{sec:approac_repcalc_basedondifference}.



% --------------------------------------------------------------
\section{Improving Reputation}\label{sec:approach_improvements}
% --------------------------------------------------------------

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.85\linewidth]	{../images/improvement_aging-crop.pdf}
    \caption{
    Schematic showing the adaptation of aging as improvement technique.
    Aging maintains a window of time in which recent claims fall.
    These remain active and counted.
    Every claim older than the limit is removed.
    }
    \label{fig:improvement_aging}
  \end{center}
\end{figure}

\subsection{Aging}

Aging's purpose is to limit the memory of the reputation system, so as to help follow changes in agent behavior or sentiment.
Older data is forgotten, so no agent is locked into a reputation they no longer deserve.
The \emph{Limit} parameter specifies how long data is retained.
Generally, \emph{Limit} can be any unit of time measurement.
The proposed implementation from Chapter~\ref{chap:implementation} divides time into rounds, and the aging limit gives a number of rounds, so it is assumed to be a natural number $\mathbb{N}$.

The schematic function of aging is shown in Figure~\ref{fig:improvement_aging}.
If the current round $r_{\mathrm{now}}$ indicates the present time, then for any claim $C^r_{m,i}$ made in round $r$, aging is:
\begin{equation}
    \begin{cases}
		\text{remove }C_{m,i} & r_{\mathrm{now}} - r > \mathrm{Limit} \\
		\text{still count }C_{m,i} & \text{otherwise}
    \end{cases}
\end{equation}
Removing a claim also removes all attached reviews on that claim.


\subsection{Weights}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.85\linewidth]	{../images/improvement_weights-crop.pdf}
    \caption{
    Schematic showing the adaptation of weights as improvement technique.
    Whichever method is used to calculate \emph{Agent N}'s reputation, it also takes the weights of the raters into account.
    Their reviews are represented according to their weights.
    How agents get their weights can differ.
    Here, weights simply reflect the agent's reputation.
    }
    \label{fig:improvement_weights}
  \end{center}
\end{figure}

Weights aim to give preference to some agents over others when aggregating reviews.
The purpose of this is to give more weights to opinions of agents who have managed to build a good reputation, and less to those with bad reputation.
Intuitively, this acts in the opposite direction than aging, because weights preserve the status quo, while aging helps to change it.

Figure~\ref{fig:improvement_weights} shows how weights are modeled.
An agent's reputation serves as its weights, calculated proportionally.
Reputation calculation methods use weighted average for aggregating reviews.


\subsection{Stakes}

We propose stakes as a new improvement method, consisting of claimers publishing a stake along their new claims.
A stake gives how certain claimers are in their valuation of their new claim, i.e. the author's review.
A high stake signals confidence of the agent in the author review's accuracy, while a low stake notifies others that even the claimer is not sure whether its author review is correct.

Stake is a similar to a self reported weight, with a scope of only the involved claim.
Other agents and even reputation calculation can use the stake when making decisions.
For example, a low stake could mean the claimer does not receive much of a reputation penalty if the author review turns out to be inaccurate, while a higher stake incurs an amplified reputation correction.

Since stake requires providing an extra valuation with every claim, it is best suited for algorithmic systems.
A sensor can provide stakes based on its known uncertainty for example.


\subsection{Other Selected Methods}

\emph{Outlier filtering} can be modeled by removing reviews which have significantly different valuations than the majority opinion.
This can be a way to prevent a small number of saboteurs to influence a claim's overall review valuation.
If the majority opinion of the claim shifts, the removed reviews need to be reconsidered and re-added if they are no longer outliers.

\emph{Anonymous ratings} serve to isolate the content from the content provider to prevent biased rating.
This is effectively implemented within the model if none of the agents use a strategy that discriminated against specific agents.

\emph{Rating range normalization} aims to even out different rating habits of agents.
For example, some agents avoid giving minimum and maximum ratings and stay at the middle ranges, others rate strong opinions more willingly.
Normalization extends the midrange rater's ratings to span the whole rating range.
This method is only applicable if agents already have a lot of ratings, because enough data is needed to confidently determine the typical rating range.






% --------------------------------------------------------------
\section{Distortion}\label{sec:distort_strategies}
% --------------------------------------------------------------
% other name: Attack Strategies

Distortion models the conscious and willing efforts of claimers to influence other's perception of their content quality.
This is one vector for agents to attack and exploit the reputation system, namely from the provider's side.
With this concept, the model allows the setup of more elaborate attack scenarios.

Distortion fits in the model as part of making new claims, seen in Figure~\ref{fig:claim_rate_process}.
When an agent $a_m$ makes a new claim $C_{i,m}$, that agent has some valuation of what the claim's quality is.
This is called the measured quality $Q_{meas,m,i}$ as per Section~\ref{sec:approach_measure_claim}.
The claimer agent $A$ needs to make an author's review $R_{author,m,i}$ about its new claim $C_{i,m}$.
Distortion basically defines how the agent comes up with the author review from the known the measured quality of the claim.
\begin{equation}
\mathrm{distort}_{a_m}(Q_{meas,m,i}) = R_{author,m,i}
\end{equation}

The most honest approach is to do no distortion at all.
Here, the author's review contains exactly the claimer's valuation of the claim quality.
Less honest agents can either state a higher or lower value than their own estimate.
To what extent and how often they do it depends on the distortion method they use.

%More complex distort schemes are the dynamic methods.



% --------------------------------------------------------------
\section{Rating}\label{sec:approach_rating_strategies}
% --------------------------------------------------------------
\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../images/claim_rate_process-crop.pdf}
    \caption{
    Schematic representation of claiming (above) and rating (below) processes, with the involved claims and reviews.
    }
    \label{fig:claim_rate_process}
  \end{center}
\end{figure}

Rating models the feedback agents leave on content they have used.
When an Agent $a_m$ rates a claim $C_{i,n}$ made by another agent $a_n$, a regular review $R_{m,i}$ is produced.

\begin{equation}
\mathrm{rate_{a_m}}(C_{i,n}) = R_{m,i}
\end{equation}

Rating is analogous to distortion in that both end up producing a kind of review.
The difference is that rating produces regular reviews on published claims, while distortion is used for author reviews.

Apart from this, rating may use a much larger amount of input data, everything that is part of or connected to the rated claim $C_{i,n}$.
This includes not only the measured quality, but also the other reviews of the $C_{i,n}$, data of the claim author agent $a_{n}$ like $a_{n}$'s reputation, other claims of $a_{n}$ and their review, and so on.
This allows modeling elaborate methods of feedback behavior.

Such amount of data is often not reasonable to utilize at once.
Therefore, models of rating usually select one valuation related to the claim, and then apply some manipulation to this value.
This input represents some numeric quality of the claim, like the author review, the quality as measured by the rater agent $a_m$, the average of other reviews, etc.
Comparing how well these perform can be a task of its own.
No specific value is assumed while discussing methods below.
The general notation $X$ is used instead.

Rating is the attack vector of those on the consumer side of transactions.
Many attacks and exploits found in related works can be modeled as rating methods, because reviews are usually the most important factor in calculating reputation.


\subsection{Simple Rate Methods}

Simple rating methods are characterized by using only one or a few input values and they can be described as a static transfer function.
The modeled ones are shown in Figure~\ref{fig:simple_rate_strategies}.
Every transfer function can have parameters and an input $X$.

The two commonly considered attack methods are \emph{promotion} and \emph{slandering}\cite{hoffman_survey_2009}.
They are modeled with linear or breakpointed linear methods, as in equations \ref{eq:rate_linear} and \ref{eq:rate_linear_brpoint}.
Linear breakpointed can work both ways, setting either a constant low or a high limit for ratings before or after the breakpoint, respectively.
Parameters: $a$, $b$, \emph{direction}, \emph{breakpoint} and \emph{breakpoint value}.
\begin{equation}\label{eq:rate_linear}
\mathrm{rate_{linear}}(C_{i,n}) = aX+b
\end{equation}
\begin{equation}\label{eq:rate_linear_brpoint}
\mathrm{rate_{breakpointed}}(C_{i,n}) = 
    \begin{cases}
      aX+b & \text{if $X$ within \emph{breakpoint}} \\
      \text{\emph{breakpoint value}} & \text{otherwise}\\
    \end{cases}
\end{equation}

Second order polynomial ratings described by (\ref{eq:rate_so}) are included from \cite{yu_detecting_2003}, where they are called exaggerated positive and negative.
Parameters: $a$, $b$, $c$ and \emph{direction}, the latter influencing if the polynomial is added or subtracted.
\begin{equation}\label{eq:rate_so}
\mathrm{rate_{second order}}(C_{i,n}) = X \pm (aX^2+bX+c)
\end{equation}

Invert rating is also adapted from \cite{yu_detecting_2003}.
\begin{equation}\label{eq:rate_invert}
\mathrm{rate_{invert}}(C_{i,n}) = -X + \mathrm{max}(X) + \mathrm{min}(X)
\end{equation}

Flattening was introduced to model the often-seen behavior of people avoiding strong opinions, tending toward the safe middle values.
This is based on a well-established observation in psychology and sociology, and is called the central tendency bias\cite{kalton_effects_1980,lee_outlier_2004}.
Flattening reduces the more extreme values towards the middle range.
The strength of flattening is given by parameter $a < 1$.

\begin{equation}\label{eq:rate_flatten}
\begin{aligned}
&\mathrm{rate_{flatten}}(C_{i,n}) = a(X-mid)+mid \\
\mathrm{where~}&mid = 0.5\big(\mathrm{max}(X) - \mathrm{min}(X) \big) + \mathrm{min}(X)
\end{aligned}
\end{equation}

Flattening can be seen as conservative or reserved rating.
The opposite of flattening is extreme rating, when an agent tends to give more extreme ratings, around the edges of the rating range.
This method uses the same transfer function as in (\ref{eq:rate_flatten}), but with a multiplier $a > 1$ that amplifies toward extremes.
Extreme rating can model agents that mostly have strong opinions, either very high or very low, a behavior that has also been observed in online rating systems\cite{schoenmueller_extreme_2018}.

Finally, the simplest of all rating methods is when nothing is done.
This means some quality $X$ of the claim is returned without altering.
An example is the method that simply repeated the author's review of the rated claim.

\begin{figure}[tbp]
  \begin{center}
    \includegraphics[width=1\linewidth]{../images/simple_rate_strategies2.pdf}
    \caption{
    Transfer functions of simple rating methods.
    The dashed line is the input without transformation.
    It can be any single value from the rated claim, for example its author review, or measured quality.
    Rating methods show example parametrizations, these can of course change.
    Both the input and ratings are selected to be within the integer interval $[1,9]$.
    }
    \label{fig:simple_rate_strategies}
  \end{center}
\end{figure}


\subsection{Random Rate Methods}
Random error rating is a method where sometimes a significantly inaccurate rating is made.
	It can model a sensor's rare erroneous data.
Also, agents whose attack approach is rate normal most of the time to build reputation and occasionally insert a malicious rating.
The malicious rating may carry more weight due to the rater's otherwise good reputation.
The method is modeled as per (\ref{eq:rate_random_error}).
Parameters: \emph{chance}.
\begin{equation}\label{eq:rate_random_error}
\mathrm{rate_{random~error}}(C_{i,n}) = 
\begin{cases}
random & \text{if $chance$ is taken by coincidence} \\
X & \text{otherwise}
\end{cases}
\end{equation}

Various other random ratings can be used to model actors who want to weak havoc or are completely ignorant.
Random, lower half, and higher half random all give a random value within the whole or part of the rating range.
\begin{equation}\label{eq:rate_random}
\mathrm{rate_{random}}(C_{i,n}) = \text{random value in $\big[\mathrm{min}(X), \mathrm{max}(X)\big]$}
\end{equation}
\begin{equation}\label{eq:rate_random_higher_half}
\mathrm{rate_{random~higher~half}}(C_{i,n}) = \text{random value in $\left[\frac{\mathrm{min}(X)}{2}, \mathrm{max}(X)\right]$}
\end{equation}
\begin{equation}\label{eq:rate_random_lower_half}
\mathrm{rate_{random~lower~half}}(C_{i,n}) = \text{random value in $\left[\mathrm{min}(X), \frac{\mathrm{max}(X)}{2}\right]$}
\end{equation}


\subsection{Rate Methods of Collectives}
Collaborating agents working towards a common goal can form collectives.
Such groups are common in real online reputation systems.
Producers often have a loyal following among consumers, who will promote the producer's content with higher ratings.
Some marketing or other agencies take clients to improve their online reputation by a coordinated promotion attack.
Producers might also initiate slandering-type attacks against competitors.
This is sometimes done by organized or spontaneous mobs, leaving baseless bad reviews on businesses or sellers for various reasons.

A main distinction is whether the collective promotes or slanders.
Slandering is typically aimed towards an agent outside the collective.
Promotion on the other hand can target members of the collective.
As also mentioned before, a collective can build up the reputation of its members by promoting each other, so that a later planned slander attack carrier more weight.

\subsection{Experience-Based Rate Methods}
Experience-based rating is when a consumer judges a content based on personal experience with it.
An agent's own experience is modeled by measuring claims, discussed in Section~\ref{sec:approach_measure_claim}.
When an agent $a_m$ measures the claim $C_{i,n}$ made by agent $a_n$, it results in the measured quality $Q_{meas,m,i}$.

Technically any of the discussed rate methods can be experience-based, if $X$ is chosen as $Q_{meas,m,i}$.
Still, explicitly defining such a rate method is worth it because rating from own opinion is common.
\begin{equation}\label{eq:rate_own_experience}
\mathrm{rate_{own~experience}}(C_{i,n}) = Q_{meas,m,i}
\end{equation}



% --------------------------------------------------------------
\section{Resolution}\label{sec:approach_resolution}
% --------------------------------------------------------------
Resolution determines what values are allowed in various parts of the model.
For example, if reviews are constrained by a resolution of 1, and minimum and maximum values are 1 and 9, then the result of ratings can be integers from 1 to 9, i.e. $\forall R$: $R\in\mathbb{Z}$ and $1 \le R \le 9$.

There are two main uses for resolution.
One of them is the already mentioned review resolution.
It applies to any review made, and its purpose is to force review values to follow the wished reputation scheme.
With separate resolution handling, rate and distort schemes remain independent of the exact selection of allowed values they are expected to produce.
This means they do not need to be customized along this, their results will simply be changed to the nearest resolution step.

The other use is with measured claim quality values.
Measured claim resolution effectively limits how finely grained agents can assess claim ground truths.
For example, let review resolution be 1 and measurement resolution be 0.5.
Here, reviews are strictly integers.
But when agents assess a claim's quality by measurement, they may get a value between two integers too.
For example, a measurement of 2.5 signifies the agent thinks the claim is better than 2 and worse than 3.
The agent will still not be able to publish a review of 2.5 because the review resolution is 1.
However, the information is there, what the agent does with it is left for the agent to decide.

The idea behind limiting measured claims in this way is the following.
When thinking of what quality a content (claim) has, it is often not possible to exactly determine it.
For example, in the 5-star scheme, one could think something is around 3 or 4 out of 5, but not give a more exact estimate.
Measuring claims models this thought process.
If measurement would give 3.4562 in this case, there is no uncertainty where exactly the quality falls.
If resolution is at 0.5, measurement can only return either 3, 3.5 or 4, or even other values if the agent has a huge inaccuracy.
A similar but slightly better claim would also measure at the same values.
This is more realistic when agents represent people judging content freely.
On the other hand, a more fine-grained resolution could be used when the scenario works with an algorithmic reputation environment.
Where agents are machines and there is an algorithmic way to assess a claim, it is reasonable to allow more freedom for claim measurements.

A third resolution domain could also be created for reputations, limiting the granularity of global reputations.
The reason this was not included is that reputation calculation methods serve as a single source of reputation values.
They can adhere to specific resolution limits as part of their calculation if needed.
For example, if a reputation strategy is build to rate in the 5-star system, it is implemented to give integers [1,5] anyway.
On the other side, a scenario can have many different kind of behaviors for agents.
Designing multiple versions of each with different resolutions unreasonable, hence the review resolution domain.



% --------------------------------------------------------------
\section{Scenarios}\label{sec:scenario}
% --------------------------------------------------------------
A \gls{scenario} is the description of a complete reputation environment including the reputation schemes and agents with various behaviors.
All parameters that are needed during simulation are part of the scenario.
Some of the most important are:
\begin{itemize}
	\item how reputation values are calculated
	\item which reputation improvement methods are applied
	\item the size of the agent population
	\item how agents behave during simulation
	\item how long the simulation goes (i.e. how many rounds)
	\item the possible rating and reputation values (e.g. binary, integers 1 to 5)
	\item what seed is used to generate random numbers
	\item what graph and data exports metrics should be created
\end{itemize}

In this way, a scenario encapsulates all that is in a single simulation.
This helps organize combinations of simulation parameters, and compare them.
For example, when the goal is to see what effect malicious agents have, one could draft separate scenarios with varying percentage of malicious agents ranging from 0\% to 100\% in each.
For a complete list of parameters with details, see Appendix~\ref{appendix:scenarios}.



% --------------------------------------------------------------
\section{Custom Evaluation Framework}\label{sec:approach_evaluation_framework}
% --------------------------------------------------------------
% aka. Simulation System
% putting together all the above

%check how to implement these
%can use existing eval frameworks? or need own one in python
%if latter, what components
%a diagram can be good also for the presentation

There have been many reputation evaluation frameworks made in the past, as surveyed in Chapter~\ref{chap:related_work}.
The first approach was to find one of these suitable for use, for which a few general requirements were applied.
These are:
\begin{itemize}
    \item available source code
    \item domain-independence
    \item preferably widely accepted
    \item extensibility and ease of modification
    \item flexibility with simulation parameters and components
    \item preferably some common reputation schemes already implemented
\end{itemize}

Most of these are needed to accommodate unique approaches and fulfill specific requirements, such as:
\begin{itemize}
    \item the use of one or more reputation improvement methods
	\item ability to implement custom reputation schemes
    \item flexible review values (custom integer ranges)
    \item second-order rating schemes (like stakes)
    \item ability to represent \glspl{claim} in some form
    \item can define custom evaluation metrics
    \item full simulation of all agents
\end{itemize}

None of the examined evaluation frameworks satisfied all needs.
Overall requirements and unique approaches warranted the inception of a new reputation simulation framework.

Because of this, a custom system was designed to simulate, compare and evaluate selected reputation schemes under various environmental conditions.
The evaluation framework uses the model described throughout this chapter so far.
The next chapter details the concrete Python implementation of the system.




% CHAPTER ######################################################################
\chapter{Implementation}\label{chap:implementation}
% ##############################################################################
%Implementation: If you did one, explain all the details here: hardware (chips, buses), OS, compiler, libraries, data structures and their field sizes.

\lettrine{S}{imulations} are done with a custom-made python simulation framework named Pyrepsys.
This chapter gives the implementation-specific details of Pyrepsys. 
For a conceptual description, see Chapter~\ref{chap:approach}.

Pyrepsys is a one-stop solution for simulating, comparing and evaluating reputation schemes.
Simulation of scenarios is performed on a round-by-round basis.
Shared configuration among scenarios, simulating batches of scenarios in succession and controlled random number generation help compare different scenarios.
Evaluation is aided by metrics.
These are data processing objects that automatically generate graphs and export data from simulations.

Flexibility and extensibility are among the main design objectives of Pyrepsys.
Variables can be altered in scenario configuration files.
The size and composition of the agent population is customizable.
New agent behaviors, metrics, reputation calculation and improvement methods can be easily added.

Miscellaneous supportive functionalities are also discussed in this chapter.
A scenario configuration creator can be used to make a batch of scenarios with variations along selected configuration parameters.
Extensive automated self-testing can verify the integrity of the simulation framework if any changes are made.
Finally, profiling and benchmarking tests are provided to identify simulation bottlenecks and measure performance.

%Additional references for using Pyrepsys are provided outside this chapter.
%For a description of the Pyrepsys command-line interface, refer to Appendix~\ref{appendix:cli_invocation}.
%For a complete list and explanation of possible scenario configuration parameters are found in Appendix~\ref{appendix:scenarios}.
% Finally, information regarding the scenario creator 

% prerequisites, dependencies, tested versions, environment

\begin{table}[tbp]
\centering
\begin{tabular}{@{}lp{0.45\linewidth}p{0.25\linewidth}@{}}
\toprule
\textbf{Module} & \textbf{Responsibility} & \textbf{Notable Classes}    \\ \midrule
agent               & Define agents with the simulation data structure & \emph{Agent}, \emph{Claim}, \emph{Review} \\
behavior            & Submodule. Define agent rate and distort strategies & \\ 
config              & Scenario configuration related. Read, parse, provide configs. Prepare other classes. & \emph{Configurator}\\
errors              & Custom exceptions & \\
helper\_types       & Accessory type definitions & \mbox{\emph{SimulationEvent}}, \mbox{\emph{ResolutionDomain}}, \mbox{\emph{LocalConfig}} \\
helpers             & Accessory function definitions. Resolution handling, Internal-Agent-Exposed conversion & \\
instantiator        & Creation of metric, reputation strategy, behavior strategy and improvement handler instances & \emph{Instantiator} \\
main                & Program entry point, preparation of logging and directories, coordination of simulation & \\
metrics             & Submodule. Defines data export and graph drawer metrics & \\
paths               & Directory paths & \\
reputation          & Submodule. Define improvement handler and reputation strategy classes & \\
results\_processor  &Containment and coordination of metrics & \emph{ResultsProcessor} \\
scenario\_creator   &Scenario creator utility & \\
scenario\_simulator &Simulate one single scenario. Aka System & \emph{ScenarioSimulator} \\
\bottomrule
\end{tabular}
\caption{
	Modules making up Pyrepsys.
	Modules \texttt{behavior}, \texttt{reputation} and \texttt{metrics} form submodules.
}
\label{tab:pyrepsys_modules}
\end{table}

% --------------------------------------------------------------
\section{Simulation Flow}\label{sec:impl_simulation}
% --------------------------------------------------------------

Simulation in Pyrepsys is built around \glspl{scenario}.
Each invocation consists of simulating one or more scenarios after each other.
At the beginning, the default scenario configuration is fetched.
Then Pyrepsys reads and applies the individual configuration for each scenario.
The scenario is simulated after these preparations.
Conducting the simulation is the responsibility of the \emph{ScenarioSimulator} class.
Once all scenarios are finished, the results processor module exports collected data and draws graphs based on the selected metrics.
The simulation part's schematic flow is shown in Figure~\ref{fig:simulation_flow}.

Scenario simulation is performed on a round basis.
The number of rounds is part of the scenario configuration.
Each round consists of four distinct phases:

\begin{enumerate}
	\item Claiming
	\item Rating new claims
	\item Applying reputation improvements
	\item Reputation calculation
\end{enumerate}

\subsection{Claiming}
Claiming is the part where agents can make new \glspl{claim}.
During this phase, all agents get an opportunity to make one new claim.
Whether an agent ends up publishing a claim or not depends on multiple factors.

First of these is the agent's \gls{claim_probability}.
This represents the likelihood of them attempting a claim if given an opportunity.
A more consumer-type agent would have a lower probability and thus claim rarely if ever.
On the other hand, producer-type agents would tend toward higher probabilities, and claim more often.
The random check whether agents attempt a claim or not is performed on the \gls{main_rng_chain}.

If the agent passes this check, a claiming process begins.
First, a claim is generated containing a random \gls{gr_truth}. 
The ground truth is not directly accessible for the agent.
It can however be measured, which is the second step of the claiming process.
Measurement results in the \gls{measured_claim_score}, which represents an approximation of the claim's true quality.
How good this approximation is, depends on the agent's ability to assess claims.
Further details of measuring claims is described in Section~\ref{sec:approach_measure_claim}.

With the measured truth, the agent executes its \gls{distort_strategy}.
It essentially applies one of the distortion methods found in Section~\ref{sec:distort_strategies}.
The resulting value is the \gls{distorted_claim_quality}.

At this point the agent checks whether the distorted quality falls in the agent's \gls{claim_range}.
Claim range serves as a minimum and maximum limit as to what claims an agent is willing to publish.
If the distorted claim quality falls outside these limits, the claim is discarded and claiming is aborted.
In this case, the agent will not claim in this round.

If the limits are not violated, the agent prepares the claim for publishing.
The \gls{author_review} is created and appended to the claim.
Its value is always the distorted claim quality from the previous step.
Finally, the agent appends the claim to the list of his claims, and so the claim is published.
The claiming process is illustrated on Figure~\ref{fig:claim_rate_process}.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../uml/simulation_flow.pdf}
    \caption{
    Simulation flow in Pyrepsys.
    A single simulation unit is the \gls{scenario}.
    Each \gls{scenario} is simulated independently after one another.
    Rounds make up the simulation within a \gls{scenario}.
    Each round consists of four distinct phases: claiming, rating, improvement and reputation calculation.
    The number of rounds is specified in the configuration.
    }
    \label{fig:simulation_flow}
  \end{center}
\end{figure}


\subsection{Rating}

Rating stage is when agents can rate claims published in the current round.
Each new claim made in the previous section is put up for rating.
For each of these claims, every agent other than the claimer is given an opportunity to rate it.
The maximum possible number of \glspl{review} made during one round is

\begin{equation}\label{eq:max_reviews}
	(N^r_{reviews}) \le N^r_{claims} (N_{agents}-1)
\end{equation}
Where $ N^r_{claims} $ and $ N^r_{reviews} $ are the number of claims and reviews made in the current round $r$.
$ N_{agents} $ is the number of agents in the current scenario.

Similarly to claiming, agents are given opportunities to rate a claim.
Whether or not they take it and leave a review depends on the agent's \gls{rate_probability}.
This percent chance represents each agent's willingness to rate encountered claims.
A low rating probability means the agent is passive and rates seldom.
A higher chance to rate represents an enthusiastic agent that often makes reviews of claims.
Each time an agent is given a chance to rate, a check is performed on the \gls{main_rng_chain}.

If this check is passed, the agent makes a review.
The schematic process of rating is shown on Figure~\ref{fig:claim_rate_process}.
It consists of executing the agent's \gls{rate_strategy} with the claim under review as input.
This is done to enable a wide range of rating methods.
The claim's other reviews, the author's review, the author's identity or the rater's own measurement of the claim \gls{gr_truth} may all play a role in determining the review value.

Once the review value is ready, a review object is created.
It is appended to the review list on the claim and also to the rater agent's list of own reviews.
With this, the review is published and the agent has finished the claiming process.
The same agent can possibly make a review in the same round again, but only when rating another claim.


\subsection{Applying Improvements}

Improvements are the third main step of rounds.
At this point, all claims and reviews of the current round have been published.
As discussed in detail in Section~\ref{sec:impl_data}, claim and review data is saved into a data structure formed by the list of agents.
This list is passed to the selected improvement methods by the \emph{ScenarioSimulator} for processing.
All modifications are made on this mutable list of agents.

Improvement methods are implemented as separate handler classes.
These handlers form a handler chain as per the \emph{chain of responsibility} software design pattern.
See Figure~\ref{fig:improvement_handlers} for an UML model.
Each handler is formed by inheriting from \emph{reputation::AbstractHandler}.
This abstract class defines the interface improvement handlers need to adhere to.
It also gives the default behavior for the handlers.
The method \texttt{handle(request)} is the default behavior of calling the next handler in the chain if any, or simply returning if the current handler is the last in the chain.
This behavior is meant to be executed at the end of all concrete improvement subclass \texttt{handle()} calls like this: \lstinline{return super().handle(agents)}.
\emph{AbstractHandler}'s second method \texttt{set\_next(handler)} is used to build the handler chain by giving which handler comes after this one is finished.
Normally this is done by the configuration module during scenario preparation.

Organizing the improvements as a handler chain has multiple benefits.
Firstly, improvements remain modular and flexible.
Separate handlers can be added to or removed from simulations.
The order of handling is free to change.
Implementing new improvements is easily done by inheriting from the abstract handler class.
Furthermore, the client code calling the handlers does not depend on which handlers are active.
This means \emph{ScenarioSimulator} is independent of improvements.
Apart from a simple structure, this allows changing the order or composition of the chain during runtime.
Although this is not currently supported, implementing this feature is simple.
Finally, by putting improvement methods in a class, they are encapsulated.
As such, there is a clear distinction between data and methods related to improvements and other parts of the system.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../uml/improvement_handlers.pdf}
    \caption{
	UML class diagram of improvement handlers. 
	\emph{Configurator} creates instances of the required improvement handlers via the \emph{Instantiator} and sets the handler chain's entry point in \emph{ScenarioSimulator}.
	When improvements are due, \emph{ScenarioSimulator} calls the first handler with the agents data, which includes all claims and reviews.
	\emph{LocalConfig} provides custom configuration for each individual improvement handler.
	This is discussed in Section~\ref{sec:local_config} in detail.
    }
    \label{fig:improvement_handlers}
  \end{center}
\end{figure}

\subsection{Reputation Calculation}\label{sec:impl_reputation_calculation}

After improvements have finished, the fourth and final step of a simulation round is calculating agent reputations.
Reputation calculation methods are implemented with the strategy software design pattern.
Each reputation scheme is implemented as a class that inherits from a common abstract parent class, \emph{reputation::ReputationStrategy}.
This class provides a simple interface containing the method \texttt{calculate\_reputations(agents)} which concrete strategies must implement.

After improvements have finished, \emph{ScenarioSimulator} starts reputation calculation by calling this method on the reputation strategy it stores.
The method takes the mutable list of all agents of the scenario as input.
Then the reputation strategy's single responsibility is calculating agent reputations based on whatever method it employs and updating each agent's \texttt{global\_reputation} attribute with it.
This holds the agent's latest public reputation score.
Before the first reputation calculation takes place, it holds an initial reputation set according to configuration before the first round.

Using the strategy pattern allows modularity and flexibility, much of the same advantages with improvement handlers.
New reputation strategies are easy to add to Pyrepsys by creating a subclass of \emph{reputation::ReputationStrategy} in the \texttt{reputation} subpackage and putting the classname on the import list.
Strategies are independent of the client class, \emph{ScenarioSimulator}.
This means they are simple to exchange.

Additionally, encapsulating reputation schemes into classes makes them self-contained and forms a clear structure.
Figure~\ref{fig:reputation_strategy} shows the UML model of reputation strategies.
implemented with the strategy pattern

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../uml/reputation_strategy.pdf}
    \caption{
	UML class diagram of reputation strategies.
	\emph{Configurator} creates an instance of the selected reputation strategy via the \emph{Instantiator} and gives it to \emph{ScenarioSimulator}.
	At reputation calculation, \emph{ScenarioSimulator} calls its reputation strategy object with the agents data, which includes all claims and reviews.
	\emph{LocalConfig} provides custom configuration for strategy instances.
	This is discussed in Section~\ref{sec:local_config} in detail.
    }
    \label{fig:reputation_strategy}
  \end{center}
\end{figure}



% --------------------------------------------------------------
\section{Agents}\label{sec:impl_agent}
% --------------------------------------------------------------

\subsection{Behavior Strategies}
Behavior strategies serve as the implementation of agent's rating or distortion methods.
The \emph{behavior} subpackage contains related classes.
Both rating and distortion strategies are implemented very similarly to reputation strategies as described in Section~\ref{sec:impl_reputation_calculation}.

The involved classes are shown in the UML diagram in Figure~\ref{fig:behavior_strategies}.
Both behavior strategy types share a common abstract ancestor called \emph{BehaviorStrategy}.
This class serves two main purposes.
First, it describes a common interface for both rate and distort strategies.
This consists of the \lstinline{execute()} function, which takes as parameter the calling agent, some data that is needed for the rating or distortion method, and an optional random seed.
Second, the common ancestor provides shared functionality for random number generation with the \lstinline{rng()} method.
This can be used by behaviors to start a \gls{branchoff_rng_chain} and generate random numbers.

Distortion and rating diverge with their own abstract adapter classes \emph{DistortStrategy} and \emph{RateStrategy}.
Both are child classes of the common ancestor class.
Their main purpose is to enable the same random chain functionality while also providing a different interface for both behavior types.
\emph{DistortStrategy} provides \texttt{distort()}, \emph{RateStrategy} provides \texttt{rate\_claim()}.
Both classes wrap \texttt{execute()} calls to their respective behavior calls.
Additionally, the adapter makes a preemptive check on the input data.
\emph{RateStrategy} allows \emph{Claim} objects to be passed, while \emph{DistortStrategy} allows \gls{measured_claim_score} values: integer and float.

Finally, concrete strategies of both types derive from the abstract adapter parents.
\emph{Configurator} creates the concrete behavior strategy instances via \emph{Instantiator} and assigns them to each agent according to scenario settings.
Agents then call upon their behavior strategies during simulation through the \texttt{execute()} interface.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../uml/behavior_strategies.pdf}
    \caption{
	UML class diagram of agent behavior strategies.
	The abstract class \emph{behavior::BehaviorStrategy} serves as a common base for both distort and rate strategies.
	Two interface classes \emph{behavior::RateStrategy} and \emph{behavior::DistortStrategy} both inherit from it.
	Concrete strategies are descendants of these two classes.
	\emph{Configurator} creates behavior concrete strategy instances via the \emph{Instantiator} and sets them on the agent using it.
	When rating or distorting, the agent calls \lstinline{execute()} on the appropriate behavior strategy.
	\emph{LocalConfig} provides custom configuration for strategy instances, discussed in Section~\ref{sec:local_config} in detail.
    }
    \label{fig:behavior_strategies}
  \end{center}
\end{figure}


\subsection{Measuring Claims}
Measuring is taking an approximation on the hidden true quality, or the \gls{gr_truth} of a claim.
It results in a value that is comparable to the ground truth or any review on the claim.
This is the \gls{measured_claim_score}.

Since assessing the quality of a claim depends on the assessing agent's know-how, experience or expertise, this is reflected in claim measurement as well.
Agents have the attribute called \gls{ctai} for this.
This represents the accuracy the agents can guess a claim's ground truth.
\Gls{ctai} is given for each agent in the scenario configuration.
The value of \gls{ctai} is the maximum error an agent can make when assessing a ground truth in each direction.
This is represented in Equation~\ref{eq:ctai}.

\begin{equation}\label{eq:ctai}
C_{gr~truth} - \mathrm{\acrshort{ctai}} <= \mathrm{measured~quality} <= C_{gr~truth} + \mathrm{\acrshort{ctai}}
\end{equation}

Whenever an agent needs to measure a claim, it calls its method \lstinline{measure_claim()}
This takes as argument the claim to be measured and a random number generator.
Since claims are measured on-demand, the generator is always a \gls{branchoff_rng_chain}.
The measurement error is created as a random value with uniform distribution between [-\acrshort{ctai},~\acrshort{ctai}].
This is then added to the claim's true quality.

\begin{equation}\label{eq:measure_claim1}
\mathrm{measurement~error = rng.uniform(-\acrshort{ctai}, \acrshort{ctai})}
\end{equation}
\begin{equation}\label{eq:measure_claim2}
\mathrm{measured~quality} = C_{gr~truth} + \mathrm{measurement~error}
\end{equation}

The above equations summarize how claim measurements are made.
Theoretically, an agent could find out the true quality of a claim by making multiple measurements.
This is prevented by doing measurements inside the \emph{Agent} class before any other parts of the code are called where the measured quality is needed.
Currently only one such case exists.
Measured quality is used as input to distortion.
The distort strategy does not get the whole \emph{Claim} object containing the ground truth.
Instead, the measurement is made before calling distortion, and passed on as argument instead of the claim's object.

A disadvantage of using a simple uniform distribution to randomize measurements is exactly its simplicity.
A uniform error can be seen as a kind of simple noise.
Other kind of random distributions or even more elaborate models of assessing the quality of a claim might be needed.
On these occasions, a viable way could be implementing a strategy pattern similar to agent behaviors and reputation strategies.

Depending on what approach a simulation takes, it might not want claims measured.
Claim measurement can be seen as involuntary distortion on the agent's part.
On the other hand, distortion represent willful manipulation of the claim's quality in the author review.
This is the default approach.
However, the claim measurement functionality is not needed when distortion strategies model all manipulation happening between spawning and publishing a claim.
For these cases, it can be turned off.
This is achieved by setting the \gls{ctai} of all agents to 0.
Every measurement will simply return the claim's original ground truth in this case.
Simulation results reflect this as if agents would operate with the ground truth itself when distorting.



% --------------------------------------------------------------
\section{Data Handling}\label{sec:impl_data}
% --------------------------------------------------------------


\subsection{Simulation Data Structure}
Pyrepsys keeps simulation data in memory during simulation.
The principle is that data is stored in hierarchical classes that represent the agents, claims and reviews.
The top level owner of all simulation data of a scenario is \emph{ScenarioSimulator}.
The structure is shown in Figure~\ref{fig:agents_data_structure}.

The list of created agents is stored by \emph{ScenarioSimulator} in a list called \texttt{agents}.
Every agent is represented by an \emph{Agent} class instance.
Agents store their own claims they made in a list called \texttt{claims}.
Claims are instances of the \emph{Claim} class.

When claims are made, the claimer agent always appends an \gls{author_review} to their claim.
This is a \emph{Review} instance and is stored in the claim object as \texttt{author\_review}.
Reviews made by agents other than the claimer after the claim is published are also \emph{Review} instances.
They are stored at two locations.
For once, they are appended to the claims they are a review of, in the \emph{Claim} object's \texttt{reviews} variable.
Second, every \emph{Agent} object stores the reviews that agent ever made in a list called \texttt{reviews}.
Although the the \emph{Review} object of a review is referenced at two locations, Python's handling of variables ensures the review object exists only once.
The drawback of this is that when removing a claim, it has to be removed from two places.
The advantage is, it is much faster and easier to find all the review a specific agent has ever made.
This is useful for some improvement methods, or possibly for rate or reputation strategies.

Both claims and reviews also store a weak reference back to their author.
Weak references are provided by Python's \texttt{weakref} module.
Objects linked with weak references are not kept alive when the all references to them are weak.
This adds minimal overhead but big benefits.
When traversing backwards from claim or review to author, the author weakref is called to retrieve the referred \emph{Agent} object.
However, were weakref not used, agents, claims and reviews would be referencing each other in a circle.
This would prevent them to be destroyed (collected) properly once the simulation is done and ScenarioSimulator empties the agents list.
Weak references prevent this, because they do not count as references when looking for whether an object is still linked from anywhere.
When only weak references remain referring to an object, garbage collection will destroy it.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../uml/agents_data_structure.pdf}
    \caption{
	UML class diagram detailing how data is stored during simulation.
	\emph{Agent} instances representing agents in the scenario are held in a list in \emph{ScenarioSimulator}.
	Each agent holds a number of claims made by that agent.
	Claims have exactly one \gls{author_review} attached to them.
	Other reviews are held in a list in \emph{Claim}.
	Agents also hold the reviews made by them in a list of their own.
	Claims and reviews link back to their authors with weak references.
    }
    \label{fig:agents_data_structure}
  \end{center}
\end{figure}

\subsubsection{Passing the Data}
All data reflects the latest state during simulation.
Improvements act on this data and modify it accordingly.
For example, the aging improvement removes old claims directly from the live agents list stored in \emph{ScenarioSimulator}.

This approach has disadvantages with certain improvements that make only temporary changes to claims or reviews.
For example, an outlier filtering removes extreme opinions.
Once majority opinion shifts toward the opinions previously deemed an outlier, they need to be re-added since they are no longer outlier.
With the approach used, it has to remove reviews from the two lists in agent and claim, and then store them until they are to be re-added.
This is not an issue in itself, because improvement handlers are implemented as classes and can store data.

Another drawback shows itself at the end of the simulation.
Here, the original unimproved state of the data that includes removed or disabled parts is not available for processing and evaluation.
Metrics solve this issue however.
This way partly the reason for how metrics were implemented as they are, called on various points of the simulation.
At each simulation event where metrics are called, they export the needed data.

The advantage of keeping the data structure always in the live state is that it remains free of clutter.
Apart from this, simulation time is not increased by having to iterate through and check for inactive or removed objects.

\subsubsection{Alternative Storage Approaches}

Some alternative approaches were also considered.
One alternative would have been storing the effects of improvements within the data structure with flags.
For example, aging would add a flag \lstinline{counted=False} to claims deemed too old.
This would be good for improvements like outlier filtering, that take objects out of consideration and possibly re-add it later.
Outlier filtering would only need to change a flag back to \lstinline{True} to re-enable a review or claim.

One drawback of this would be that all the temporarily removed data objects would still remain in the live data lists.
They would be iterated over many times during rounds, increasing simulation time.
To prevent this, removed objects could also be stored in separate lists.
In this case however, the approach is mostly identical to what was implemented anyway.

Another drawback is that the simulator needs to check for the various flags introduced.
There can be many flags, like \lstinline{counted}, \lstinline{removed}, \lstinline{boosted}, \lstinline{weakened} etc.
Once checks for flags are built into Pyrepsys, improvements would no longer be modular, because code specifically tailored to them would remain in the simulator constantly.

\vspace{6pt}

Another approach to organize the data would have been storing different states of the data.
For example, also store a version without any modification by improvements, just the raw data that was generated by agents.
In parallel, the actively improved version is also stored.

Generally, such data duplication is disadvantageous.
It takes more memory and is harder to manage.
Processing time also increases.
Apart from this, the unimproved raw data would tell a false picture.
For example, it would store all claims for all rounds, without telling some claims were removed by aging.
Or store all reviews for all rounds, even though a couple of them would have been filtered inactive for some rounds.
Improvements also have a feedback effect on the data generated in subsequent rounds.
For example, a review of a claim removed by filtering can possibly change how another agent rates the claim.
Only seeing the raw data that includes the filtered review, the decision (rating) process of the other agent could not be reconstructed properly.

So the two datasets could be analyzed strictly together only.
This would be problematic, since the live view would hold the data from the last round only.
There is no information about what happened when in rounds before the last.

\subsubsection{Data in Metrics}
Metrics are a special case of data storage.
As mentioned above, one of the reasons they came to be was that data often needs to be saved along the time domain.
An example is following the change of reputations in every round.
For this purpose, metrics are hooked into points of simulation like the end of rounds and end of scenarios.
These are called simulation events.
With this, any snapshot of the data is possible.

Metrics get access to simulation data structure discussed above.
Specifically, they receive the list of agents stored by \emph{ScenarioSimulator}.
From it, they are free to take and store data as they wish.
Because metrics are implemented as classes, they can store data of their own.

The drawback of this approach is that data is stored multiple times after all, in the metrics.
This takes up some additional memory.
However, metrics rarely store a copy of the raw data only.
Most often they do some kind of (pre)processing already at storage.
For example, calculate an average of something per round, or make aggregates of agent data, etc.
This justifies their separate data storing.
When time-based raw data does need to be stored by metrics, it is only done on a need basis.
Meaning when that metric is not needed, it is turned off and the system is no longer strained by holding unneeded data.


\subsection{Internal and Agent-Exposed Values}\label{sec:i_ae_reprepentation}
Reviews, claim qualities and reputations can take up any value depending on the reputation scheme.
The simplest case is when reviews are either positive or negative.
This might be represented as 0 and 1.
In other cases, discrete values are needed, like \emph{good}, \emph{ok} and \emph{bad}.
Finally another common case is when values are integers in a range.
For example, the 5-star system using 1, 2, 3, 4 and 5 as possible values is common.
By default, Pyrepsys also uses this approach with values between 1 and 9.
This is however, decided by the scenario configuration.

The need for flexibility in reputation schemes necessitates a generic storage solution of values in Pyrepsys.
This is the reason values are stored differently internally than they are displayed and used by agents.
\Gls{ae_representation} is when a value is in the state facing the agents, the reputation system and display or export functions.
This is what is mentioned in examples above.
\Gls{i_representation} is the way Pyrepsys stores values in the internal data structures.

Internally stored values are in the range [0,~1].
Agent-exposed values are controlled by scenario settings.
The scenario configuration parameters \emph{MIN\_RATING} and \emph{MAX\_RATING} determine the possible range.
In the code, whenever a variable stores internal values, it end with '\texttt{\_i}', while variables of agent-exposed values end with '\texttt{\_ae}'.

Every time a value is fetched from or is saved to another state of representation, it is converted.
Conversion is done by two functions in the \texttt{helpers} module. 
These are \lstinline{agent_to_internal()} and \lstinline{internal_to_agent()}.
Both have a shorter alias in the form of \lstinline{a2i()} and \lstinline{i2a()}.
These functions take a single value $v$ as input and return the converted equivalent in the other representation. 
Conversion follows equations \ref{eq:a2i} and \ref{eq:i2a}.
\begin{equation}\label{eq:a2i}
v_{internal} = \frac{v_{ae} - \mathrm{minimum~rating}}{\mathrm{\gls{rating_span}}}
\end{equation}
\begin{equation}\label{eq:i2a}
v_{agent~exposed} = (\mathrm{minimum~rating}) + (\mathrm{\gls{rating_span}}) * v_{i}
\end{equation}

Used together with resolution, it is possible to reproduce schemes like the example ones at the beginning of this section.
See sections \ref{sec:approach_resolution} and \ref{sec:impl_resolution} for details on resolution handling.
Table~\ref{tab:rating_scheme_examples_pyrepsys_config} shows how the scenario should be configured for common schemes.
Agent-exposed representation gives numeric values even when they are meant as labels or other non-numeric choices.
In these cases, behavior and reputation strategies and metrics have to take care of interpreting them as such.

\begin{table}[tbp]
\centering
\begin{tabular}{@{}llccll@{}}
\toprule
\multirow{2}{*}[-0.2em]{\textbf{Scheme}} & \multirow{2}{*}[-0.2em]{\textbf{Rate Values}} & \multicolumn{2}{c}{\textbf{Rating Settings}} & \multicolumn{2}{c}{\textbf{Representations}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6} 
                        &                                  & \textbf{Range}    & \textbf{Resolution} & \textbf{Internal}    & \textbf{External}   \\ \midrule
Binary          & +, -                     & {[}0,1{]}                & 1               & 0, 1 & 0, 1           \\
Labels & good, ok, bad       & {[}1,3{]}                & 1               & 0, 0.5, 1 & 1, 2, 3          \\
5-star          & 1, 2, 3, 4, 5            & {[}1,5{]}                & 1               & 0, 0.25, 0.5, 0.75, 1 & 1, 2, 3, 4, 5            \\ 
Precise & 1, 1.1 ... 4.9, 5 & {[}1,5{]}                & 0.1         & 0, 0.025 ... 0.975, 1 & 1, 1.1 ... 4.9, 5         \\
\bottomrule
\end{tabular}
\caption{
	Example rating schemes and Pyrepsys scenario configuration implementing them.
	The last two columns show how possible values are represented in the system.
	Using rating range and review resolution all common schemes can be configured.
	External refers to agent-exposed representation.
}
\label{tab:rating_scheme_examples_pyrepsys_config}
\end{table}


\subsection{Resolution}\label{sec:impl_resolution}

Resolution settings control what exact values are allowed for select variables during simulation.
This section discusses implementation details.
See Section~\ref{sec:approach_resolution} for conceptual discussion of resolutions.

\subsubsection{Domains and Conversion Points}
Pyrepsys defines two resolution domains.
Each of these represent a resolution constraint for a part of the claiming, rating or reputation process.
When a value that is part of a process enters a domain, it is converted to the configured resolution.
Figure~\ref{fig:resolution_domains} shows which domains apply to what parts of the claiming and rating processes.

Domains determine the smallest allowed step for selected variable values.
The two scenario parameters that control the two domains are \texttt{REVIEW\_RESOLUTION} for reviews and \texttt{MEASURED\_CLAIM\_RESOLUTION} for measurements.
Both of these expect the value of the smallest allowed step given in \gls{ae_representation}.

There are three transformation points as also seen on Figure~\ref{fig:resolution_domains}.
\begin{enumerate}
    \item on the result of a claim quality measurement, before the agent sees the measurement in its rating or distort strategy
    \item after the execution of a rating strategy on the returned review, before publishing the review
    \item after the execution of a distort strategy on the returned distorted quality, before publishing as author's review
\end{enumerate}

Reviews and distorted qualities are results of the customizable behavior strategies of agents. 
Conversion of these results is done without any punishment for agents that do not prepare their values for the configured review resolution.
All reviews are saved with this resolution, whether an agent's review or a claimer's author review.

Code implementing resolution handling is found in the \texttt{helpers} module.

\subsubsection{Resolution Conversion}

When a value that is part of a process enters a domain, it is converted to the configured resolution.
The main function for converting between resolution domains is \lstinline{convert_resolution()}.
It expects two arguments.
First, \texttt{number} holds the value to be converted.
Second, \texttt{target\_resolution\_domain} gives which resolution it should convert to.
This is given in the form of an enum defined in \texttt{helper\_types}.
The purpose of \lstinline{convert_resolution()} is to call the \lstinline{find_nearest_step()} with the appropriate arguments to find which step the value to convert falls nearest to.

The function \lstinline{find_nearest_step()} accepts \texttt{number} holding the value to be converted and \texttt{steps\_sorted\_list}, a list sorted by ascending order and containing all the allowed steps of the chosen resolution domain.
It is this function's job to effectively convert to the new resolution by selecting a possible step from the list based on the value of \texttt{number}.
For this, it uses a bisection algorithm \lstinline{bisect_left} from Python's \texttt{bisect} module.

Left bisection efficiently searches for an element in a sorted list.
Originally it is meant to be used for efficiently inserting new items into a sorted list.
Given a sorted list and a new item, bisect returns the index where the item should be inserted into the list to keep it sorted.
Since it uses binary search for finding this, its efficiency is $O(logN)$.
The difference between left or right bisection is if the element to insert is already in the list, should the returned insertion index be left or right of the existing equivalent element(s).

With the point of a would-be insertion found, \lstinline{find_nearest_step()} decides on the returned resolution step.
The index returned by left bisection is denoted by \lstinline{i}.
If the input \texttt{number} coincides with resolution step \lstinline{steps_sorted_list[i]}, that is returned.
If not, then \texttt{number} is between \lstinline{steps_sorted_list[i]} and \lstinline{steps_sorted_list[i-1]}.
The distances from both are calculated as seen below.

\begin{lstlisting}
diff_left = round(abs( steps_sorted_list[i-1] - number ), 8)
diff_right = round(abs( steps_sorted_list[i] - number ), 8)
\end{lstlisting}

Rounding is needed to correct small floating-point errors that can result from working with small numbers.
The step with the smaller distance is returned.
If the distances are equal, the larger, right-neighboring step is returned.

\subsubsection{Creating Resolution Steps}\label{sec:impl_create_steps}

Ordered lists of possible resolution steps are created for every resolution domain.
These are called step lists or resolution ladders.
The step lists are sorted by ascending order and are created during scenario configuration by the function \lstinline{_configure_system_resolutions()} in the \lstinline{helpers} module.
The method is simply starts with the smallest value, increments it with the resolution's step and adds it to the step ladder.
This is repeated until the maximum allowed value is reached.
Each step list is stored in a module-level variable in \lstinline{helpers}.

The function \lstinline{_configure_system_resolutions()} is an exception from all others related to configuration, which are located in the \lstinline{config} module in the \emph{Configurator} class.
The reason for putting this function in \lstinline{helpers} instead of \lstinline{config} is to avoid a circular import dependency between the two modules.
An alternative solution was to keep the functionality in the \emph{Configurator} class.
In this case, the resolution steps would have to be requested via the \lstinline{get()} method of \emph{Configurator}, or with a similar call.
This adds an overhead, and since resolution conversion is a very common operation, so it can build up to a noticeable factor.
Because of this, the resolution ladders are kept locally in \lstinline{helpers}.

To allow (re-)calculation of the resolution steps every time the scenario configuration changes, \emph{Configurator} must let the \lstinline{helpers} module know when this happens.
This is solved with callbacks from \emph{Configurator}.
The function for calculating resolution ladders is registered with \emph{Configurator}, which calls it every time the scenario configuration is updated.
For more discussion on configuration updated callbacks, see Section~\ref{sec:config_update_callbacks}.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../images/resolution_domains-crop.pdf}
    \caption{
    Resolution domains in various stages of rating (top) and claiming (bottom).
    (a) Ground truths have no limitation and can take up any value within the \gls{rating_span}, stored as \gls{i_representation}.
    (b) Claim quality measurements are produced in the \emph{measured claim resolution domain}.
    (c) In their distorting or rating strategies, agents can compute an output without regard to resolution constraints.
    (d) Once the produced values are published, they are converted into the \emph{review resolution domain}.
    }
    \label{fig:resolution_domains}
  \end{center}
\end{figure}



% --------------------------------------------------------------
\section{Random Number Generation}\label{sec:impl_rng}
% --------------------------------------------------------------

Python's \lstinline{random} module is used for generating random numbers.
Many parts of Pyrepsys uses random numbers for simulation.
These include:
\begin{itemize}
	\item Ground truth of new claims
	\item Random error on claim quality measurements
	\item Checks against an agent's claim or rate probability
	\item Some agent behavior strategies
\end{itemize}

In order to facilitate comparability and repeatability of simulations, Pyrepsys implements repeatable random number generation.
Python's \lstinline{random} rolls its internal state every time a random number is taken from it.
If the state of the generator is known and can be restored, random number sequences can be recreated.
Such sequence of random numbers are called random chains here.

Repeating the chain of a random number generator is possible by seeding it.
Scenarios provide a configuration parameter called \lstinline{seed} for this purpose.
The class \emph{ScenarioSimulator} owns an instance of the random number generator created with the \lstinline{random} Python module.
Before simulation of a scenario, the generator is seeded with the above parameter.
This generator provides the so-called \gls{main_rng_chain}.
It serves random values for checks and uses where the same number and type of random value is needed regardless of what execution path the simulation takes.
These are for example checks whether agents will claim or not, provide a review or not, and providing random seeds for throwaway random chains.

\Glspl{branchoff_rng_chain} or throwaway chains are supplementary random number sequences.
These are used when it is not known in advance how many or what type of random values are needed, or whether any will be needed at all.
Such cases are claiming and rating randoms, or agent behavior strategies.

Throwaway chains are made by branching-off the main chain.
That means a random value is rolled from the main generator, and used as seed for the throwaway chain.
To provide a constant usage of the main chain, every time there is a possibility that a branch-off chain will be needed, a random number will be generated for its seed.
This way the chain is used equally regardless of the scenario's settings.

An example illustrating the purpose of the two random chains is claiming.
Claiming requires three random values: 
\begin{itemize}
	\item one for the claim probability check to see if the agent will attempt a claim
	\item one for generating a ground truth (quality) in the new claim
	\item one for the measurement error when the agent assesses the claim's quality
\end{itemize}
The first one of these must be generated whether the agent claims or not.
The second two only if the check to claim succeeds and claiming is attempted.
Changing the number of agents or agent's claim probabilities would modify which agents enter the the claiming process, and so the agents claiming afterward the shift would get different random values than before.
To prevent this, each claim opportunity uses exactly two random values off the main chain: one for the probability check, and another as a seed for the branch-off random chain.
If the agent passes the probability check, the generated seed is used to create the branch-off random sequence.
The random values for the new claim's quality and the measurement error are taken from this throwaway chain, which is destroyed after the claiming.
In this manner, all agents receive the same inputs from the simulator even between scenarios, provided that the order of the agents stays the same.



% --------------------------------------------------------------
\section{Configuration}\label{sec:impl_config}
% --------------------------------------------------------------

This section discusses topics related to configuration in Pyrepsys.
It also covers some scenario parameters, but not all.
For a usage-oriented description of every possible scenario parameter, see appendix~\ref{appendix:scenarios} instead.


\subsection{Scenario Definition}
\Glspl{scenario} are expected as \texttt{.yaml} files located in the \texttt{scenarios} folder.
Each file contains a list of parameters in yaml format, telling what the scenario is about.
Parameters include the reputation strategy, used improvement handlers, agents, enabled metrics, resolution, rating range, the initial seed and other settings.

%Pyrepsys allows describing scenarios without a full specification.
%This means not all needed parameters are listed, just some selected ones.
%The missing parameters are taken from a default scenario configuration.
%This is useful when the effect of one or more parameter changes are needed, since those can be simply listed alone in their scenario configuration.
%For all conceptual discussions, a scenario refers to all the simulation environment fully specified, regardless of where they come from in the implementation.
Scenario files can either be fully or partially defined.
Full definition requires the scenario give an explicit value for all possible parameters.
Partially defined scenarios contain only a subset of all settings.

Scenario files can fill two different roles when running Pyrepsys.
First, regular scenarios are the separate simulation units that will be simulated after each other.
These are given as a list to simulate, typically more than one per invocation.
Second, the default scenario is used as a fallback for parameters while simulating the list of regular scenarios.
Only one scenario file can be given as default scenario every Pyrepsys invocation.
The two kinds of scenarios form a two-level system.
Whenever a regular scenario is simulated and a parameter is requested, the system checks among the parameters from the scenario file.
This is called the active configuration.
If a parameter is not defined there because the scenario is not fully defined, then the default scenario serves as fallback.
The requested parameter is returned from the default configuration.
For this reason, the scenarios used as default scenario must always be fully defined.

Defining scenarios with this two-level configuration system is simpler.
Usually only one or at most a few parameters are varied between scenarios to see the isolated effect of the changes.
Here, the default scenario should contain the baseline values for scenario parameters, fully defined.
Then, each specific simulated scenario can be partially defined only containing parameters that differ from the default configuration.
Pyrepsys also provides a scenario creator utility, which uses the same approach to create scenarios with combinations of given parameter values.
%See appendix~\ref{appendix:sc} for documentation.

\subsection{Reading Scenario Files}
Reading configuration from scenario files to memory is done by the \emph{Configurator} class.
It provides two separate methods for reading active and default scenarios.
Both are wrappers for the same class method responsible for reading scenario files, named \lstinline{_config_from_file_to_memory()}.
After opening and reading the file contents, it calls on the third party \texttt{pyyaml} module to read the contents into a dictionary.
A name for the scenario is added to the dictionary by taking the filename without the \texttt{.yaml} extension.
The function also generated the hash of the scenario file it just read using the sha256 algorithm.
This will be logged to provide a quick check whether two scenarios of the same name contain the same settings.

\subsection{Configuration Storage Locations}
Multiple locations are responsible for storing configurations in Pyrepsys.
\emph{Configurator} is the main source of settings.
Apart from that, locally stored config collections also exists, and there is one local configuration cache.

\subsubsection{Configurator}
\emph{Configurator} is located in the \texttt{config} module and is responsible for most configuration-related functionality.
Its main tasks are reading scenario files, preparing other modules and classes for simulation, and providing configuration parameters when requested.

The class method \lstinline{get()} can be used to request config values from \emph{Configurator}.
It takes an argument with the name of the config parameter.
This function implements the two-level configuration retrieval.
First the active config is checked, and then the default if the active configuration does not contain the requested parameter.

\emph{Configurator}'s storage of active scenario settings is changed between scenarios, while the default config remains the same for the whole duration of a Pyrepsys run.

\subsubsection{Local Configuration of Strategies and Handlers}\label{sec:local_config}
Some behavior or reputation strategies, improvement handlers or even metrics are parametrized and so can have configuration values attached to them.
An example for how these are defined in scenario files is found below.
\begin{lstlisting}
improvement_handlers:
  - name: "Aging"
    limit: 6
  - "Weights"
\end{lstlisting}
Here, aging is defined with one parameter called \texttt{limit}.
When configuration is attached in this way, the original param value is replaced with a dictionary, and the \texttt{name} param is used to hold what was just a string before.
For comparison, the \texttt{Weights} improvement handler uses no parameters in the above example, so only the implementig class' name is needed as a string.

Strategies, handlers or metrics that are extended with parameters like this are interpreted by the \emph{Configurator} class in a method called \lstinline{_unpack_extended_config_list()}.
After an instance of the appropriate class is created, it receives a dictionary containing its parameters if there are any.
The storage solution is the class \emph{LocalConfig}, from which all strategies, handlers and metrics inherit.
This class provides a method similar to \emph{Configurator}'s \lstinline{get()} for accessing the local config.
These are then accessible from within the class methods of strategies, handlers or metrics.

Storing these parameters with the class instances instead of a central repository like \emph{Configurator} allows instantiation of different versions of the same class.
Simply the local config needs to be specified with different values.

\subsubsection{Configuration Caches}
There is one occasion where a Pyrepsys module uses a local cache for scenario configuration that normally would be requested from \emph{Configurator}.
This caching appears in the \texttt{helpers} module.
Before its introduction, the conversion functions between \gls{i_representation} and \gls{ae_representation} made multiple config param requests each.
These are very frequently used functions, because they are called every time an internally stored value is fetched, or when an agent-generated value is stored.
While \emph{Configurator}'s \lstinline{get()} is fast, the sheer amount of calls made incurred a noticeable overhead.
When this local cache was introduced, it resulted around 70\% improvement in the cumulative time spent in the function that converts from internal to agent-exposed representation.
This can mean around minute that was shaved off. 
Other functions in helpers also benefit from speed improvement, albeit to a lesser degree.

Caching works by storing needed configuration parameters and precalculated values in the \texttt{helpers} module.
Module functions can then access the cache dictionary instead of making a request from the \texttt{config} module.
\emph{Configurator} updates the cache via a callback each time the scenario configuration is modified.


\subsection{Update Callbacks}\label{sec:config_update_callbacks}
\emph{Configurator} allows registering callbacks for configuration changes.
These are called every time the default or active scenario configuration changes.
Other parts of Pyrepsys can use this to be notified of changes in configuration.

The method to register the callback functions is \lstinline{register_config_updated_callback()}.
Every time a new scenario file was read and parsed, \emph{Configurator} executes all registered callbacks.
Update callbacks are used by the local config cache in \texttt{helpers} and the resolution step ladder creator method.
Refer to Section~\ref{sec:impl_create_steps} for why resolution handling is solved this way.


\subsection{Setting Up Simulations}
The \texttt{main} module serves as an entry point and coordinates the Pyrepsys classes to perform simulations.
After preparatory tasks like logging setup and creating the output folder, instances of \emph{ScenarioSimulator}, \emph{ResultsProcessor} and \emph{Instantiator} are created.
The system-wide \emph{Configurator} instance is also requested.
The \texttt{config} package provides \lstinline{getConfigurator()}, which returns the same instance of \emph{Configurator} every time it is called.
\emph{ScenarioSimulator} receives \emph{ResultsProcessor}, the configurator gets the \emph{Instantiator} instance and the output directory path.

Now, the scenario files can be read and the objects can be configured.
First, the default scenario settings are read.
Then, for each scenario, its file is read, \emph{ScenarioSimulator} is configured, \emph{ResultsProcessor} is configured, then begins the simulation.
This repeats until all scenarios were simulated.

The scenario simulator's setup is divided into three parts: improvement handlers, reputation strategy and agents.

\subsubsection{Reputation Strategy and Improvement Handlers}
Reputation strategy and improvements have a similar setup.
First, the params storing them are requested.
Then, a method called \lstinline{_unpack_extended_config_entry()} parses parameters that are extended with local configuration described in Section~\ref{sec:local_config}.
\emph{Instantiator} is used to create reputation and improvement handler object instances.
The instances receive their local config is any was set for them, and they are assigned to the \emph{ScenarioSimulator}.
For the improvement handlers, the improvement chain is tied together in order of appearance in the scenario file, and the scenario simulator only receives the first handler.

\subsubsection{Agents}
For setting up agents, the agent base behavior definitions also need to be parsed first.
Base behaviors are template settings for agent definitions, defined in scenario files under the parameter \lstinline{agent_base_behaviors}.
Each of them must contain all possible agent parameters, along with the base behavior's name.
For example:
\begin{lstlisting}
agent_base_behaviors:
  - name: 				"HonestAccurateRater"
    distort_strategy: 	"DistortDoNothingStrategy"
    rate_strategy: 		"RateFromOwnExperience"
    claim_range: 		[0, 1]
    claim_probability: 	0
    rate_probability: 	1
    claim_truth_assessment_inaccuracy: 0.0625
\end{lstlisting}
Base behaviors can then be used when defining agents. Below, the above definition is used to define two agent types. Both of them will have 10 agents created of the type. The second type overwrites the base behavior's \lstinline{claim_probability} of 0 with 0.2.
\begin{lstlisting}
agents:
  - amount: 			10
  	base_behavior: 		"HonestAccurateRater"
  - amount: 			10
  	base_behavior: 		"HonestAccurateRater"
  	claim_probability: 	0.2    # overwrite claim% from the base behavior
\end{lstlisting}
The typical usage method for base behaviors is defining a set of base behaviors in the default scenario's file, and then using these definitions in other scenario files that will be simulated.
Otherwise, \lstinline{agent_base_behaviors} also adheres to the two-level configuration system.
If it is re-defined in an active scenario's file, that definition overwrites the one in the default scenario settings.

Each agent type's rate and distort strategies are created in a similar manner to reputation strategies and improvement handlers.
The local config parameter extensions are parsed, then instances are created.
With the agent's configuration parameters and behavior strategies ready, the \emph{Agent} instances are created.

This is done by \emph{ScenarioSimulator}, with its method for this task called \lstinline{create_agents()}.
Agent objects are saved by scenario simulator into the agents list.

\subsubsection{Metrics and ResultsProcessor}
Metric instances are special in the regard that they are persistent between all simulated scenarios of a Pyrepsys invocation.
This is because metrics usually compare multiple scenarios, and they need to be able to collect and store data from each of them.
\emph{ResultsProcessor} stores metric instances for this reason.

During configuration, \emph{Configurator} checks if \emph{ResultsProcessor} already stores instances of the listed metrics of scenarios.
If not yet, the same process from earlier sections involving local configs and the \emph{Instantiatior} is used to create the missing metric instances.
\emph{ResultsProcessor} receives them and will keep them until Pyrepsys terminates.

Including or excluding a metric from data collection during a scenario is determined by the \lstinline{metrics} parameter in scenario files.
The two-level system of default and active configurations applies to it like to any other parameter.
The typical usage is listing metrics used in the default scenario's file, and not defining any metrics in the simulated scenarios.
This results in the same default metrics being used for all scenarios.
If any scenario defines the \lstinline{metrics} parameter, \emph{only the metrics found in this definition} will be used for the scenario, since it completely overwrites the one in the default.
To add an extra metric along the defaults to a scenario, it needs to be listed explicitly with the default metrics as well.
To exclude one compared to the default definition, all other not-excluded metrics need to be listed.



% --------------------------------------------------------------
\section{Results Processing}\label{sec:impl_results_processing}
% --------------------------------------------------------------
Pyrepsys offers optional built-in results processing to form a full pipeline from simulation to data analysis and visualization.
The \lstinline{results_processor} and \lstinline{metrics} modules can collect, process and export simulation data as graphs or tables.
\emph{Metric} classes define what data is collected, and how it is processed and exported.
\emph{ResultsProcessor} contains the metrics, notifies them of events and funnels simulation data to them.
Together, metrics and \emph{ResultsProcessor} form a publish-subscribe architecture.
The UML diagram of these classes is found in Figure~\ref{fig:reproc_metrics}.


\subsection{Simulation Events}

Pyrepsys defines four simulation events as listed in Table~\ref{tab:simulation_events}.
Each time execution reaches one, the event is triggered by notifying the results processor of it.
Then, \emph{ResultsProcessor} notifies the metrics of events that concerns them.

Two events have fixed consequences.
The \lstinline{BEGIN_SCENARIO} event calls every active metric's \lstinline{prepare_new_scenario()} method, which is used to make preparations before the scenario starts.
\lstinline{END_OF_SCENARIO} calls the \lstinline{export()} method of active metrics, which is used to make post-processing on the data, assemble graphs or tables, and export them into files.

The two other events are \lstinline{END_OR_ROUND} and \lstinline{END_OR_SCENARIO}.
Each activated metric subscribes to none, just one or both of these two.
Whenever these events trigger, the \lstinline{calculate()} method of subscribed metrics is called by \emph{ResultsProcessor}.
This is the time metrics can save data from the simulation system and do pre-processing on it.

Which of the two events a metric subscribes to is determined by what kind of reporting the metric does.
In any case, simulation data is passed for \lstinline{calculate()} for both events in the form of the agents list, the same one saved by \emph{ScenarioSimulator}.
Depending on which event it is, the just ended round's number or the ended scenario's name is also passed.

\begin{table}[tbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Event}        & \textbf{Called Metric Method} \\ \midrule
BEGIN\_SCENARIO       & \texttt{prepare\_new\_scenario() } \\
END\_OF\_ROUND        & \texttt{calculate()} if registered \\
END\_OF\_SCENARIO     & \texttt{calculate()} if registered \\
END\_OF\_SIMULATION   &  \texttt{export()} \\
\bottomrule
\end{tabular}
\caption{
	Simulation events in Pyrepsys.
	\emph{ResultsProcessor} is notified of events from other parts of the code.
	It notifies metrics based on what event was triggered and which metrics are active.
	The second column shows which methods of active metrics are called.
}
\label{tab:simulation_events}
\end{table}


\subsection{Metrics}

Generally there are two types of metrics: data exporters and graph plotters.
The distinction is not strict however, and metrics can even have both functions at the same time.
It makes sense to divide metrics along what kind of reporting they do.
When a metric calculates a specific kind of value, statistic or aggregate, it can make sense to produce graphs and data exports from the same metric, since the data is available anyway.

Every metric is defined as a child of the abstract \emph{metrics::Metric} class, as seen in Figure~\ref{fig:reproc_metrics}.
Then, selecting metrics for usage is done through the scenario configuration by giving its classname.
The initialization method contains a metric's name and its events of interests.
Additionally, every metric implements at least the three previously discussed functions.
Graph plotting metrics in Pyrepsys use \texttt{matplotlib}, while data exporters use Python's \texttt{csv} module.
Exported files are saved to an invocation-specific output directory called the \gls{artifacts_dir} by default.

Using metrics with Pyrepsys is optional.
The costs of using them is the extra memory needed for storing collected data, and the additional processing time for dealing with the data and creating the export formats.
The memory footprint obviously depends on how big of a simulation data is amassed and how many separate metrics store parts from it.
Processing time spent on metrics was observed to be tens of seconds when simulation takes a few minutes.

The benefit of metrics is the convenience of automatic reports and graphs exported based on simulation data.
Flexibility and extensibility is also fulfilled.
Adding new metrics is as easy as with any other behavior or reputation strategy, or improvement handler.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=1\linewidth]	{../uml/reproc_metrics.pdf}
    \caption{
	UML class diagram of the results processor and metrics.
	Metric instances are created by \emph{Configurator} via \emph{Instantiator}.
	\emph{ResultsProcessor} registers metrics according to what event they are listening to.
	\emph{ScenarioSimulator} and the \lstinline{main} package trigger events on the results processor.
	Metrics are notified of events that interest them.
	\emph{LocalConfig} provides custom configuration for each individual metric.
	This is discussed in Section~\ref{sec:local_config}.
    }
    \label{fig:reproc_metrics}
  \end{center}
\end{figure}



% --------------------------------------------------------------
\section{Other Facilities}\label{sec:impl_misc}
% --------------------------------------------------------------

\subsection{Automated Self-Testing}
Pyrepsys contains an automated testing suite.
It can be used to verify the integrity of functionality under the test coverage.
Tests can also be added to perform sanity checks of specific components, like metrics.
Such tests contain scenario configuration that is known to produce an expected and explainable results.
Finally, the test suite contains tests that profile and benchmark the simulator.
Using these the most time-consuming parts of the simulation can be identified.

Testautomation is provided by Pytest.
Tests reside in the \texttt{tests} directory in Python files.
Each of them is a function and are found automatically by Pytest.
To allow some control over what tests to run, Pyrepsys uses Pytest's mark feature.
Some tests are marked with the identifiers found in Table~\ref{tab:pytest_marks}.

\begin{table}[tbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Mark}        & \textbf{Meaning} \\ \midrule
perf       & Test does profiling or is a benchmark \\
manual     & Test needs manual evaluation of results \\
long       & Test takes a longer time than most others \\
\bottomrule
\end{tabular}
\caption{
	Pytest test markers in Pyrepsys.
	Relevant tests are flagged with them.
	Marks can be used to select which tests to run or exclude from running.
}
\label{tab:pytest_marks}
\end{table}


\subsection{Logging}
Pyrepsys logs notable events and information in a logfile saved to the \gls{artifacts_dir}.
This is implemented using Python's \lstinline{logging} module.

Logging is set up by \texttt{pyrepsys.main}.
Messages are logged into the logfile called \texttt{simulation.log}.
They are also displayed on the console in a shortened format.
Each Pyrepsys module gets its own logger with the \lstinline{logging.getLogger()} method.
It returns the logger after the \texttt{name} parameter provided to it, which is chosen as the name of each Pyrepsys module.

Messages of different severity are logged, including debug, info, warning and error.
By default, only info level messages or more severe are displayed.
If debug messages are needed, the default level can be overwritten in the \texttt{pyrepsys.main} module.
It is also possible to change the severity of selected Pyrepsys modules only.
This is beneficial when debugging a specific module.




% CHAPTER ######################################################################
\chapter{Evaluation} \label{chap:evaluation}
% ##############################################################################

\lettrine{T}{his} chapter presents experiments performed with Pyrepsys.
First, the used metrics are introduced.
Then, the various experiment are described, first the parameters and expectation, then the results of simulations.
Finally, a discussion formulates takeaways from the conducted simulations.

% --------------------------------------------------------------
\section{Metrics}\label{sec:metrics}
% --------------------------------------------------------------
%Metrics: First define which quantities you will measure and why you chose those.

Effectiveness is measured as a global, system-wide accuracy, as opposed to an agent-utility approach.
This is chosen because measuring individual agent's success makes less sense, since agents can not decide who they transact with.

The main metric for system accuracy is reputation scores compared to claimer's inaccuracy.
Claimers who publish accurate and reliable author's reviews should have a good reputation, while, inaccurate author's reviews should result in a lower reputation.
This way, an ideal linear relationship exists between inaccuracy and reputation, where the lowest possible claiming inaccuracy should map to the highest reputation, while the theoretical largest inaccuracy in the worst possible reputation.
This ideal line is shown as a dashed diagonal in plots of this metric.
If an agent is above the line, it managed to cheat the system by getting a better reputation than its inaccuracy and honesty would proportionally warrant.
If under, the agent is punished harder than it should be under straight proportions.

Total claiming inaccuracy is determined by two factors: measurement inaccuracy and distortion.
Since measurement is not under the agent's control, it is also called the involuntary inaccuracy.
Distortion is fully each agent's doing, so it is voluntary and also called dishonesty.
\begin{equation}
\text{Total Claiming Inaccuracy} = \text{Measurement Inaccuracy} + \text{Dishonesty}
\end{equation}
This is computable for a claim $C_A$ made by agent $A$ using the claim's author review and its \gls{gr_truth}.
\begin{equation}
\text{Total Claiming Inaccuracy}(C) = \left| Q_{true,C_{A}} - R_{authorA,C_{A}} \right|
\end{equation}
Averaging every claim of an agent gives the \emph{Average Total Claiming Inaccuracy} of that agent.

\vspace{6pt}

\emph{Average Inaccuracy of Raters} measures how close regular reviews fall to the ground truths of rated claims.
It is calculated as
\begin{equation}
\frac{\sum_{C\in\text{all claims}}({(\text{average of review on }C) - Q_{true,C})}}{\text{number of all claims}}
\end{equation}

This metric is most influenced by the setup of agents and their behavior strategies.
For this reason it can be used less to judge the effectiveness of reputation or improvement schemes, and more to see how the agent population rates.
When more dishonest or malicious raters are present, this metric will indicate that with a larger average inaccuracy.



% --------------------------------------------------------------
\section{Setup}\label{sec:setup}
% --------------------------------------------------------------
%Setup: Clearly describe which method you use to obtain your values and how the setup looks like. It is probably one of three:
%    Analysis: provide a formal/mathematical analysis of your approach (and related work). Which models / equations do you use? Why? How? 
%    Simulation: Run your approach in a simulation framework. Which simulator (version) runs on which hardware (specs)? Which parameters? Why?
%    Experiment: Measure values from a real implementation. Which devices are used? Conditions? Parameters?

Experiments are set up and simulated with Pyrepsys.
Each one consists of multiple scenarios with one or two selected parameters that are varied.
Most other parameters are kept the same to allow clear comparison.
Fixed parameters are
\begin{itemize}
	\item rating range: $[1,9]$
	\item rating resolution: 1
	\item measurement resolution: 0.5
	\item reputation strategy: \lstinline{BasedOnAvgDifferenceOfClaimsAndReviewsWithWeight}
	\item initial reputation: 5
	\item number of rounds: 80
	\item seed: \lstinline{pyrepsys_random_seed}
\end{itemize}

\begin{table}[tbp]
\centering
% comment for the old, long table
%\begin{tabular}{lrrllll}
\begin{tabular}{@{}lll@{}}
\toprule
%\multirow{2}{*}[-0.2em]{\textbf{Base Behavior}} & \multirow{2}{*}[-0.2em]{\textbf{Rate\%}} & \multicolumn{2}{c}{\textbf{Claim}}            & \multirow{2}{*}[-0.2em]{\textbf{\acrshort{ctai}}} & \multicolumn{2}{c}{\textbf{Behavior}}             \\ \cmidrule(lr){3-4} \cmidrule(l){6-7} 
% &  & \textbf{\%} & \textbf{Range} &  & \textbf{Distort} & \textbf{Rate}  \\ \midrule
%\texttt{HonestClaimer} & 0.1 & 1 & [0.5,1] & 0.125 & \texttt{DoNothing} & \texttt{DoNothing} \\
%\texttt{DishonestClaimer} & 0.1 & 1 & [0.5,1] & 0.125 & \texttt{MaxSometimes} 50\% & \texttt{RateFromOwnExperience} \\
\multirow{2}{*}[-0.2em]{\textbf{Base Behavior}} & \multicolumn{2}{c}{\textbf{Behavior Strategy}} \\ \cmidrule(lr){2-3}
 & \multicolumn{1}{c}{\textbf{Distort}} & \multicolumn{1}{c}{\textbf{Rate}} \\ \midrule
\texttt{HonestClaimer} & \texttt{DoNothing} & \texttt{DoNothing} \\
\texttt{DishonestClaimer} & \texttt{MaxSometimes} (0.5) & \texttt{RateFromOwnExperience} \\
\texttt{HonestRater} & \texttt{DoNothing} & \texttt{LinearFromClaimerReputation} (7)\\
\texttt{InfluencedHonestRater} & \texttt{DoNothing} & \texttt{BetweenAuthorReviewAndExperience} \\
\texttt{MidrangeRater} & \texttt{DoNothing} & \texttt{Flatten} (0.5) \\
\texttt{MaliciousRater} & \texttt{DoNothing} & \texttt{RateInvertedSlope} \\
\texttt{HonestClaimerRater} & \texttt{DoNothing} & \texttt{RateFromOwnExperience} \\
\texttt{DishonestClaimerRater} & \texttt{MaxSometimes} (0.5) & \texttt{LowrateHonestClaimers} \\
\bottomrule
\end{tabular}
\caption{
	Agent base behaviors used as template in simulations.
	Values in parenthesis show parameters for behavior strategies that need them.
	\texttt{MaxSometimes} has \emph{chance} of claiming the highest quality.
}
\label{tab:agent_base_behaviors}
\end{table}

Varied parameters are the improvement techniques and agent population composition.
Agent base behaviors shown in Table~\ref{tab:agent_base_behaviors} are defined as templates.
Apart from the behavior strategies, the default agent parameters are
\begin{itemize}
	\item \gls{ctai}: 0.125
	\item claim range: [0,1], or [0.5,1] for  \texttt{HonestClaimer} and \texttt{HonestClaimerRater}
	\item claim probability: 0.8 for claimers, 0 for only raters
	\item rate probability: 0.8 for raters, 0.1 for only claimers
\end{itemize}

Experiments are each identified with a letter.

\subsubsection{Experiment A: effect of aging}
This experiment varies two parameters, both over three possible settings, making nine scenarios total.
First, improvements are varied as 1) no improvements, 2) aging with limit 25 and 3) aging with limit 10.
Then, the number of \texttt{MaliciousRater} agents are varied as 1) none 2) 30 and 3) 60.
\texttt{HonestRater} and \texttt{MidrangeRater} extend the agent population so that there are always 60 raters with these two in equal numbers.
Lastly, all scenarios include 10 \texttt{HonestClaimer} and 10 \texttt{DishonestClaimer} agents.

\subsubsection{Experiment B: malicious raters}
In this experiment, the ratio of dishonest raters is increased gradually.
Since reputation calculation is based on the honesty of claim valuations in the form of author reviews, dishonest raters who otherwise never claim will not get a reputation adjustment.

The experiment has 10 \texttt{HonestClaimer} and 10 \texttt{DishonestClaimer} agents, and a total of 60 rater agents.
These are made up of 1) none 2) 20 3) 40 and 4) 60 \texttt{MaliciousRater} agents.
\texttt{HonestRater} and \texttt{MidrangeRater} agents fill the remaining rater spots equally, until all scenarios have 60 raters.

The four versions are simulated with and without the weights improvement enabled, totaling at 8 scenarios.

\subsubsection{Experiment C: dishonest claimers}
This experiment has three versions, each a sub-experiment.
The first one (C/I) increases the ratio of dishonest claimers.
All scenarios have 60 raters, 20 of each type: \texttt{HonestRater}, \texttt{MidrangeRater} and \texttt{InfluencedHonestRater}.
The \texttt{HonestClaimer} to \texttt{DishonestClaimer} ratio is changed as 1) 10:0 2) 8:2 3) 2:8 and 4) 0:10.
The weighted and weightless versions are simulated additionally, totaling at 8 scenarios.

Since a third of the agents are influenced by author reviews in their rating decisions, claimers are expected to be able to manipulate these agents into a better rating than the claim's deserved quality.
However, the other agents are also honest and quite numerous, so combined they are expected to keep the dishonest claimers at bay.

The second (C/II) experiment has the rater types \texttt{HonestRater} and \texttt{MidrangeRater} removed, so only 20 \texttt{InfluencedHonestRater} remain as raters.
Compared to (C/I), the rater, or consumer agent population becomes more susceptible to influence.
This means they take author reviews of claims more seriously overall.
These agents are expected to be more easily manipulated, so dishonest claimers can possibly get a better reputation than before.

The third (C/III) version fixes the ratio of honest and dishonest claimers, 10 agents each type.
However, the 10 \texttt{DishonestClaimers} make dishonest claims increasingly often: 1) never 2) 50\% 3) 85\% and 4) 100\%.
This aims to find out whether there is an upper limit to frequency of dishonest claimers still tolerated.
Similar to (C/II) only 20 \texttt{InfluencedHonestRater} agents take part as raters.
However, their rate strategy is varied between 1) the usual for this agent type \texttt{RateBetweenAuthorReviewAndExperience} and 2) \texttt{RateFromOwnExperience}.
This changed the normally influenceable raters to make rate decisions purely from their own opinion.

\subsubsection{Experiment D: effect of weights}
This experiment also has three sub-experiments and it aims to find out the effect of weights as improvement technique.
Because weights are calculated from reputations, both claimers and raters should get reputation adjustments.
For this, all agents need to be claimers, including the raters.
This is often not the case in reputation systems for people, but is common for algorithmic reputation environments, where agents are machines.

(D/I) uses \texttt{HonestClaimerRater} and \texttt{DishonestClaimerRater} agents in the numbers 1) 18:2 2) 14:6 3) 10:10 and 4) 2:18. Improvement techniques are varied as 1) Aging with limit 30 and 2) Aging with limit 30 and Weights.

(D/II) makes honest raters dogmatic, in that they will tendentially give higher reviews for claims that come from good-reputation agents, and lower for claims with lower-reputation authors. 
This is the \texttt{LinearFromClaimerReputation} rate strategy with limit of 7. 
Limit gives the point above which reviews are amplified higher, while below it they are lowered.
Similarly, dishonest agents become selfish saboteurs. 
They will always give the highest possible author reviews (\texttt{MaxSometimes} with chance 100\%).
When rating, they will rate everything with the lowest score, using \texttt{LowrateAll}.
Improvements and honest-dishonest numbers stay the same.

(D/III) changes honest raters from dogmatic to rating based on their own experience.
Reputation plays no role in their reviews. 
Everything else stays the same.

\subsubsection{Experiment E: measurement accuracy}
This experiment is for finding out the influence of measurement accuracy.
The \gls{ctai} of raters is configured as 1) zero 2) 0.125 3) 0.25 and 4) 0.375.
A \gls{ctai} of 0.125 increment means 1 full score in \gls{ae_representation} in the integer rating range [1,9].

Every scenario has 5 \texttt{HonestClaimer} and 5 \texttt{DishonestClaimer} agents, both who are set not to rate at all.
Raters use the rate strategy \texttt{RateFromOwnExperience} and are created in the amounts 1) 10 and 2) 1.
The purpose of this is to find out whether more agents are able to combat the bigger inaccuracy better than only a single agent can.



% --------------------------------------------------------------
\section{Results}\label{sec:results}
% --------------------------------------------------------------
%Results: visualize your results as graphs and compare to the values of related work.

\begin{figure}[]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/a/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment A: effect of aging.
    Aging increases toward the right column, while malicious raters increase toward the bottom row.
    }
    \label{fig:res_a_scatter}
  \end{center}
\end{figure}

\begin{figure}[]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/a/ReputationsPerRounds_joined.pdf}
    \caption{
 	Progression of agent reputations per rounds in experiment A: effect of aging.
    Aging increases toward the right column, while malicious raters increase toward the bottom row.
    }
    \label{fig:res_a_reps}
  \end{center}
\end{figure}

\begin{figure}[]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/a/AvgAccuracyPerRound.pdf}
    \caption{
    Averaged cumulative rater inaccuracy per rounds in experiment A: effect of aging.
    In the legend, first index increases with more aging, the second with more malicious agents.
    }
    \label{fig:res_a_rateinaccuracy}
  \end{center}
\end{figure}


\subsubsection{Experiment A: Effect of Aging}

Figures~\ref{fig:res_a_scatter},~\ref{fig:res_a_reps} and \ref{fig:res_a_rateinaccuracy} show the results of the aging experiment.
The claiming inaccuracy diagram on Figure~\ref{fig:res_a_scatter} shows that more malicious agents pull every agent's reputation down from the ideal line.
The Figure shows the two distinct groups of honest and dishonest claimers.
Each dot is a claimer agent.
Honest claimers form the groups on the left in each subfigure.
They have better reputation than the other group, which are the dishonest claimers.

In this experiment, the effect of aging brings no improvement for reputation, in fact it makes reputation scores more spread out for groups.
This is observable on Figures~\ref{fig:res_a_scatter} and \ref{fig:res_a_reps}.
The former shows that agent groups are less and less concentrated with more aging, while Figure~\ref{fig:res_a_reps} shows that reputations become more erratic with more aging.
This can be explained by the smaller amount of data available for averaging and other aggregations with aging.
Since aging removes old claims and all involved reviews that are older than the configured limit, this data is lost as far as reputation calculation and review averaging goes.

Figure~\ref{fig:res_a_rateinaccuracy} illustrated the rater's inaccuracy. Since raters do not claim, and their rating methods do not need old claims, they are less affected. 
However, since \texttt{HonestRater} agents use the reputation of rated claim authors, the fluctuation of reputation scores also influence their ratings. 
This is seen as swings in inaccuracy on the aged scenarios.

\subsubsection{Experiment B: Malicious Raters}

\begin{figure}[]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/b/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment B: malicious raters.
    Malicious raters increase toward the bottom row.
    The left column is without any improvements, the right side is with weights enabled.
    }
    \label{fig:res_b_scatter}
  \end{center}
\end{figure}

Increasing the number of malicious raters results in a similar effect as observed in the previous experiment, only without the interference of aging.
Figure~\ref{fig:res_b_scatter} shows that malicious raters push the reputation of all claimers equally down.

Honest claimers are smeared towards their ideal reputation levels, as they form a sort of vertical line.
This means their inaccuracy was the same, still they received different reputations.
On the bottom row with all the raters malicious, the spread of honest claimer reputations is a little over 2 points.

Because malicious raters never claim, they do not get reputation adjustments and stay on the default of 5 for the whole simulation.
This means there is no way to filter their ratings, even with weights and a number of honest raters still in the system.

\subsubsection{Experiment C: Dishonest Claimers}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/c/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment C/I: dishonest claimers.
    The number of dishonest claimers increases downwards.
    Left column: no improvements, right: weights enabled.
    }
    \label{fig:res_c_scatter}
  \end{center}
\end{figure}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/c/ReputationsPerRounds_joined.pdf}
    \caption{
 	Progression of agent reputations per rounds in experiment C/I: dishonest claimers.
    The number of dishonest claimers increases downwards.
    Left column: no improvements, right: weights enabled.
    }
    \label{fig:res_c_reps}
  \end{center}
\end{figure}

C/I results are shown on figures \ref{fig:res_c_scatter} and \ref{fig:res_c_reps}.
Dishonest claimers are positioned above their exactly ideal reputation, but the difference is insignificant.
Good-intentioned raters are able to keep order even when all claimers are dishonest.
Figure~\ref{fig:res_c_reps} shows however, that it takes around 20-30 rounds for reputations to settle.
Both figures show no improvements on the left columns and weights on the right side.
There is no difference between the two versions.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/c2/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment C/II: dishonest claimers.
    The number of dishonest claimers increases downwards.
    Left column: honest raters and claimers have the default \gls{ctai}, right: \gls{ctai} is tripled for them.
    }
    \label{fig:res_c2_scatter}
  \end{center}
\end{figure}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/c2/AvgAccuracyPerRound.pdf}
    \caption{
    Averaged cumulative rater inaccuracy per rounds in experiment C/II: dishonest claimers.
    In the legend, first index means lower \gls{ctai} if 0, higher if 1, applied only to honest raters and claimers.
    The second index increases along with the number of dishonest claimers.
    }
    \label{fig:res_c2_rateinaccuracy}
  \end{center}
\end{figure}

Experiment C/II removes all raters that are not influenceable, i.e. that are not \texttt{InfluencedHonestRater}.
It also tests the effect of higher \gls{ctai} for honest claimers and raters both.
As seen on figures ~\ref{fig:res_c2_scatter}, dishonest claimers are able to get an even better reputation by influencing raters with high author reviews.
In the right column, honest claimers have more inaccuracy, so they are more spread out horizontally on the total claiming inaccuracy axis.

Rater inaccuracy is also affected by the higher \gls{ctai}, shown on Figure~\ref{fig:res_c2_rateinaccuracy}.
As expected, a higher \gls{ctai} makes reviews more inaccurate.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/c3/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment C/III: dishonest claimers.
    Dishonest claimers make exaggerated maximum quality claims more often towards the bottom row.
    Left column: raters are influenced by author reviews, right: raters rely on their own experience only.
    }
    \label{fig:res_c3_scatter}
  \end{center}
\end{figure}

C/III simulates dishonest claimers that are dishonest increasingly often.
As observed on Figure~\ref{fig:res_c3_scatter}, dishonest claimers slide rightwards on the subfigures, meaning they make more and more dishonest claims.
With influenceable raters, the dishonest claimers are able to acquire a better reputation.
In fact, the undeserved reputation gains get relatively larger as more and more dishonest claims are made.
However, if raters disregard the author's review (right, Figure~\ref{fig:res_c3_scatter}), reputation scores stay on the ideal line regardless of dishonest claims.

\subsubsection{Experiment D: Effect of Weights}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/d/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment D/I: weights.
    The ratio of dishonest claimer-raters increases downwards.
    Both columns have aging with limit 30, right side also has weights.
    }
    \label{fig:res_d_scatter}
  \end{center}
\end{figure}

All agents are both raters and claimers in this experiment, because this is needed for them to have reputation adjustments.
D/I increases the number of dishonest agents.
Bad-intentioned raters are no longer malicious, instead they rate honest agents dogmatically with a lower score.
As expected, honest agent reputations are brought down, seen in Figure~\ref{fig:res_d_scatter}.
Using weights makes no difference here.

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/d2/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment D/II: weights.
    The ratio of dishonest claimer-raters increases downwards.
    Both columns have aging with limit 30, right side also has weights.
    Honest raters are dogmatic.
    }
    \label{fig:res_d2_scatter}
  \end{center}
\end{figure}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/d2/ReputationsPerRounds_joined.pdf}
    \caption{
 	Progression of agent reputations per rounds in experiment D/II: weights.
    The ratio of dishonest claimer-raters increases downwards.
    Both columns have aging with limit 30, right side also has weights.
    Honest agent reputations gradually recover with weights.
    }
    \label{fig:res_d2_reps}
  \end{center}
\end{figure}

D/II makes agents dogmatic and divisive.
Honest agents give better ratings to good reputation claimers and worse for bad reputation ones.
Dishonest agents claim dishonest every time.
Figure~\ref{fig:res_d2_scatter} shows that honest agents are able to recover their good reputation with the help of weights.
This is a relatively long process, taking 30-60 rounds (Figure~\ref{fig:res_d2_reps}).

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/d3/AvgTotClaimInaccuracyAndReputationScatter_joined.pdf}
    \caption{
    Average Total Claiming Inaccuracy and reputation scatter diagram of experiment D/III: weights.
    The ratio of dishonest claimer-raters increases downwards.
    Both columns have aging with limit 30, right side also has weights.
    Honest agents rate from their own experience.
    }
    \label{fig:res_d3_scatter}
  \end{center}
\end{figure}

In comparison, D/III results show that the same recovery of honest agent recovery no longer happens when the honest agents are not dogmatic.
On Figure~\ref{fig:res_d3_scatter} all agents fall below the deserved ideal reputation line.
Weights pushes reputations closer to ideal, and all agents receive this effect.

\subsubsection{Experiment E: Measurement Accuracy}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/e/ReputationsPerRounds_joined.pdf}
    \caption{
 	Progression of agent reputations per rounds in experiment E: measurement accuracy.
    \gls{ctai} increases downwards.
    Left: 10 raters, right: only 1 rater.
    }
    \label{fig:res_e_reps}
  \end{center}
\end{figure}

\begin{figure}[tbp]
  \begin{center}
        \includegraphics[width=0.75\linewidth]	{../results/e/AvgAccuracyPerRound.pdf}
    \caption{
    Averaged cumulative rater inaccuracy per rounds in experiment E: measurement accuracy.
    In the legend, first index means there are 10 raters if 0, only 1 rater if 1.
    The second index increases along with \gls{ctai}.
    More raters can balance measurement inaccuracy.
    }
    \label{fig:res_e_rateinaccuracy}
  \end{center}
\end{figure}

Increasing the measurement inaccuracy (\gls{ctai}) of agents is expected to have the effect of more inaccurate ratings, both author and regular.
This was partially shown in experiment C/II as well.
Here, inaccuracy is varied even more.

Figure~\ref{fig:res_e_rateinaccuracy} shows that rater inaccuracy generally increases with \gls{ctai}, as expected.
When 10 agents rate, the average inaccuracy is limited, while a single agent rates much more inaccurately.
Time makes sense, as the inaccuracies average out for more agents.
Similar is observed in Figure~\ref{fig:res_e_reps}, where 10 completely inaccurate agents (bottom left) can set the reputations within 30 rounds, while a single agent with the same inaccuracy (bottom right) fails to arrive at the same reputations in 80 rounds.
Inaccurate raters can find out the reputation of agents accurately when they are in big numbers.



% --------------------------------------------------------------
\section{Discussion}\label{sec:discussion}
% --------------------------------------------------------------
%Discussion: Explain where, why, and to which extend you have or have not improved over related work. What does this mean for your initial problem question?


\subsubsection{Simulation Framework}
The created Pyrepsys framework fulfilled expectations, but not without remarks.
Pyrepsys does not currently model agent decision making, for example there is no way for agents to decide who they interact with.
This is the basis of measuring agent success as metric, as stressed by the Alpha Testbed covered in Chapter~\ref{chap:related_work}.
However, such functionality would be possible to implement.

Another weakness, which is also an advantage, is generality.
When working within a concrete domain, a more niche evaluation framework can model the reputation environment better, especially when the domain's main differentiator is on the communication protocol or network level.
For example, wireless sensor networks use reputation system to decide routing, and routing outcomes feed back to reputation (or trust) scores.
The TRMSim-WSN simulator was build specifically for this purpose, to simulate reputation or trust-based routing\cite{marmol_trmsim-wsn_2009}.

Pyrepsys is able to work without specifics of domains, and this is good when needed.
So far no universal and cross-domain evaluation frameworks gained widespread acceptance and use, even though researchers introduced multiple previously.
Simulating different kind of environments was possible and produced many of the expected results.
The simulator also manages to stay flexible and extendable, so it provides a viable alternative to other previous systems.


\subsubsection{Aging}
Aging was shown to make the reputation system's memory shorter.
While this was not particularly useful in tested experiments with static behaviors, it can have its uses.
When agent behaviors or qualities change, a reputation system should recognize this and adjust the agents reputation accordingly.
Examples for such change of behavior can be switching to a different claiming method, or the agent gets more accurate or honest over time, or the other way around.
This is a realistic, since static behavior is rarely the case, even algorithmic agents can change abruptly with an update for example.
Aging's leniency to forget agent history could make a reputation system react faster to changes.

When selecting the aging time limit, the goal is to still produce more or less stable aggregations, so that reputation does not become erratic, as happened on Figure~\ref{fig:res_a_reps}.
At the same time, the system should still allow changes over some time frame.
Setting the aging limit needs to be tested for this in each different environment.

\subsubsection{Weights}
Weighting was shown to be helpful, but only under very specific configuration.
The weights implementation simply takes the agent's reputation and gives a proportional weight.
Since only claimers get reputation, purely rater agents never adjust their starting reputation, and so nor do they get weights.
When these same agents rate, their reviews will mostly be taken equally, because they all have default weights.

This was not the case with an agent population representing an algorithmic environment, where most raters are claimers as well.
Most other cases have this problem in the used model.

Either the weights method or the reputation calculation could change to resolve this.
If a reputation scheme can give reputation to raters who do not claim, the same simple weights can be used.
If the weight calculation is different, non-claiming raters should get weights assigned as well.

%a few dishonest can infiltrate, many dishonest can bring down honest reputations and get similar reps themselves
%note that dishonest agents do not cooperate here. if they were, they would probably be able to do better than anyone else


\subsubsection{Reputation Calculation}
\lstinline{BasedOnAvgDifferenceOfClaimsAndReviewsWithWeight}, the reputation calculation method used, was unable to handle scenarios with a high percentage of raters who never claim.
Such strict consumers are often seen in human-used reputation systems, where most users never enter the system as producers.
The lack of reputation assignment for consumer-only raters enables malicious or dishonest behavior for them, because there is no basis to filter, weight, etc. their reviews.

This could be remedied with a different reputation method and more elaborate agent behavior strategies.
If a method assigns reputation to agents based on their reviews as well, pure raters can no longer evade reputation penalties or rewards.

Alternatively, to get the already used reputation calculation method to work on rater side as well, the model would need some kind of second order rating, where reviews are rated as well.
Such systems exist already, for example online markets sometimes ask if a review was helpful or not.

%In cases where explicit claim quality measurement is possible, an automatic feedback mechanism could compare reviews the later measured with how the claim prediction turned out, and raters that predict well are rewarded




% CHAPTER ######################################################################
\chapter{Conclusion} \label{chap:conclusion}
% ##############################################################################
%Conclusion: Summarize main points; formulate key message.
%    Remind the readers what they have read and why it was significant. Don’t give new explanations, just the facts.
%    Repeat the most important result (number!) and what it means for the problem.
 
%* At the end of this chapter or in the conclusion chapter there should be a final summary of all observed effects and their implications.
 
\lettrine{W}{e} successfully set up a generic reputation system model and provided a Python implementation of it named Pyrepsys.
Pyrepsys is the first evaluation framework to support modular reputation improvement methods.
We collected a selection of promising techniques for improving reputation system accuracy, and building robustness against attacks and malicious behavior.
Two of these were evaluated with Pyrepsys.

In cases were agent behavior never changes, aging was shown to be useless or possibly even counterproductive.
Due to forgetting older data, it increases the spreads of aggregations like review averages and reputation calculation.

Weighting worked only in specific circumstances, namely when agents are both claimers and raters at the same time, and rating is dogmatic, meaning honest agents rate better reputationed agents with disproportionately better ratings, and vice versa.
Weights did not bring an improvement when most raters never claimed.
We attribute this to the used reputation calculation method, which assigns reputation values based on honesty in claiming, so that agents who never claim do not get reputation adjustments.

All in all, accurately modeling reality within a reputation system is hard.
The diversity of agents in intent, behavior and ability, and many circumstances of domains need to be accounted for.
The difficulty rises even more if the aim is to stay generic.



% --------------------------------------------------------------
\section{Outlook}\label{sec:outlook}
% --------------------------------------------------------------

Many opportunities remain both in the introduced model and implementation, and in trust and reputation research.

The Pyrepsys model and implementation can be continued by
\begin{itemize}
	\item Evaluating more improvement methods
	\item Including complex threats and attack methods
	\item Implementing known reputation calculation methods
	\item Exploring dynamic agent behavior and agent collaborations
\end{itemize}

As for trust and reputation systems, researchers have been aiming to create a generic and widely accepted evaluation framework for decades now, with only partial and intermittent successes.
Reputation systems are deployed in many different environments, each environment with its own circumstances.
Because of this, it is difficult to find an approach that fits all.
Nonetheless, as more and more people and devices join networks, the need for reputation systems will increase, necessitating ways to compare different approaches.




\begin{appendix}

% CHAPTER ######################################################################
\chapter{Scenario Configuration Options}\label{appendix:scenarios}
% ##############################################################################

Table~\ref{tab:all_scenario_options} lists top-level scenario parameters.
Agent-specific parameters are listed in Table~\ref{tab:all_agent_options}.

\begin{table}[p]
\centering
\begin{tabular}{@{}lp{0.175\linewidth}p{0.4\linewidth}@{}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Function}    \\ \midrule
\texttt{reputation\_strategy} & string \mbox{(classname)} & Reputation calculation method \\
\texttt{improvement\_handlers} & list of strings \mbox{(classnames)} & Active improvement techniques \\
\texttt{seed} & any or null & Seed for random number generation \\
\texttt{agent\_base\_behaviors} & agent definition & Agent type templates \\
\texttt{agents} & agent definition & Concrete agents in the scenario \\
\texttt{metrics} & list of strings \mbox{(classnames)} & Active metrics \\
\texttt{MIN\_RATING} & float & Bottom of the rating and reputation range \\
\texttt{MAX\_RATING} & float & Top of the rating and reputation range \\
\texttt{INITIAL\_REPUTATION} & float & Starting reputation for agents \\
\texttt{SIM\_ROUND\_MAX} & integer $>0$ & Number of rounds to simulate \\
\texttt{MEASURED\_CLAIM\_RESOLUTION} & float & Resolution of measured qualities \\
\texttt{REVIEW\_RESOLUTION} & float & Resolution of published reviews \\
\bottomrule
\end{tabular}
\caption{
	All top-level scenario configuration options in Pyrepsys.
}
\label{tab:all_scenario_options}
\end{table}

\begin{table}[p]
\centering
\begin{tabular}{@{}lp{0.25\linewidth}p{0.25\linewidth}@{}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Function}    \\ \midrule
\texttt{distort\_strategy} & string \mbox{(classname)} & Distortion strategy \\
\texttt{rate\_strategy} & string \mbox{(classname)} & Rating strategy \\
\texttt{claim\_range} & list of two floats & Min. and max. quality the agent publishes claims with  \\
\texttt{claim\_probability} & float $\in [0,1]$ & Chance of claiming in a round \\
\texttt{rate\_probability} & float $\in [0,1]$ & Chance of leaving a review \\
\texttt{claim\_truth\_assessment\_inaccuracy} & float $\in [0,1]$ & \gls{ctai} (measurement inaccuracy) \\
\texttt{amount} & integer $>0$& Create this many concrete agents of the given type \\
\texttt{name} & string & Name of an agent base behavior. \\

\bottomrule
\end{tabular}
\caption{
	All agent configuration options in Pyrepsys.
}
\label{tab:all_agent_options}
\end{table}




% CHAPTER ######################################################################
%\chapter{Command Line Interface and Invocation}\label{appendix:cli_invocation}
% ##############################################################################




% CHAPTER ######################################################################
%\chapter{Scenario Creator}\label{appendix:sc}
% ##############################################################################

\end{appendix}




% ##############################################################################

% Abbreviations
%	\printglossary[type=\acronymtype,title=Abbreviations]

% Glossaries 
\printglossary[type=acronym]       % print only acronyms

\printglossary

%\printglossaries                        % print all glossaries
%\printnoidxglossary[sort=word]
%\printnoidxglossaries


% ##############################################################################

%\nocite{*}   % use this to print all references

% print bibliography
% check if biblatex or legacy bibtex is used
\ifoptionbiblatex
    \printbibliography[heading=bibintoc]              % print BibLaTex bibliography
\else
    \addcontentsline{toc}{chapter}{Bibliography}
    \bibliographystyle{ieeetr}      % bibtex style
    \bibliography{references}       % bibtex library filename(s) WITHOUT extension
\fi


\end{document}


